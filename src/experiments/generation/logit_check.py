#!/usr/bin/env python3
import os
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
import pickle
from dotenv import load_dotenv
from vllm import LLM, SamplingParams
import torch
from transformers import AutoTokenizer
from tqdm import tqdm

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')
BASE_DIR = os.getenv('BASE_DIR')

class LogitChecker:
    def __init__(self, 
                 language: str,
                 model_type: str,
                 trial_num: str,
                 model_name: str,
                 tensor_parallel_size: int = 1,
                 max_tokens: int = 32,
                 max_model_len: int = 4096):
        """
        Args:
            language: ì–¸ì–´ ì½”ë“œ (en/fr/ko/ja)
            model_type: ëª¨ë¸ íƒ€ì… (local/openai)
            trial_num: í”„ë¡¬í”„íŠ¸ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸
            model_name: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
            tensor_parallel_size: GPU ë³‘ë ¬ ì²˜ë¦¬ í¬ê¸°
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            max_model_len: ìµœëŒ€ ëª¨ë¸ ì…ë ¥ ê¸¸ì´
        """
        self.language = language
        self.model_type = model_type
        self.trial_num = trial_num
        self.model_name = model_name
        
        # vLLM ì„¤ì •
        self.tensor_parallel_size = tensor_parallel_size
        self.max_tokens = max_tokens
        self.max_model_len = max_model_len
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
        self.model = None
        self.tokenizer = None
        
        # í”„ë¡¬í”„íŠ¸ì™€ ë°ì´í„°
        self.prompt_template = None
        self.source_data = None
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.output_dir = Path(f"{BASE_DIR}/sound-symbolism/data/processed/art/logits")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def load_prompt(self):
        with open(f"{BASE_DIR}/sound-symbolism/analysis/experiments/prompts.json", "r", encoding="utf-8") as f:
            prompts = json.load(f)
        self.prompt_template = prompts["generation"][self.trial_num][self.language]["user_prompt"]

    def load_source_data(self):
        data_file = f"{BASE_DIR}/sound-symbolism/dataset/1_preprocess/nat/{self.language}_data.json"
        with open(data_file, "r", encoding="utf-8") as f:
            self.source_data = json.load(f)

    def load_model(self):
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        
        # vLLM ëª¨ë¸ ì´ˆê¸°í™”
        self.model = LLM(
            model=self.model_name,
            tensor_parallel_size=self.tensor_parallel_size,
            max_model_len=self.max_model_len,
            trust_remote_code=True
        )
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def get_logits_for_generation(self, prompt: str) -> tuple[str, np.ndarray]:
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=self.max_tokens,
            return_logits=True  # logit ê°’ ë°˜í™˜ ì„¤ì •
        )
        
        # í…ìŠ¤íŠ¸ ìƒì„± ë° logit íšë“
        outputs = self.model.generate(prompt, sampling_params)
        generated_text = outputs[0].outputs[0].text.strip()
        
        # logit í–‰ë ¬ êµ¬ì„±
        logits = []
        for output in outputs[0].outputs:
            if hasattr(output, 'logits'):
                logits.append(output.logits)
        
        logits_matrix = np.array(logits)
        
        # ìƒì„±ëœ ë‹¨ì–´ ì¶”ì¶œ
        if '`' in generated_text:
            word = generated_text.split('`')[1]
        else:
            word = generated_text
        
        return word, logits_matrix

    def run(self, max_samples: Optional[int] = None):
        # í•„ìš”í•œ ë°ì´í„° ë¡œë“œ
        if not all([
            self.load_prompt(),
            self.load_source_data(),
            self.load_model()
        ]):
            return
        
        # ë°ì´í„° ì¤€ë¹„
        meanings = [(item.get('definition', ''), item.get('word', '')) 
                   for item in self.source_data]
        
        if max_samples:
            meanings = meanings[:max_samples]
        
        results = []
        
        # ê° ì˜ë¯¸ì— ëŒ€í•´ ë‹¨ì–´ ìƒì„± ë° logit ê³„ì‚°
        for meaning, original_word in tqdm(meanings, desc="ë‹¨ì–´ ìƒì„± ë° logit ê³„ì‚°"):
            prompt = self.prompt_template.format(meaning=meaning)
            word, logits = self.get_logits_for_generation(prompt)
            
            if len(word) > 0 and logits.size > 0:
                results.append({
                    'original_word': original_word,
                    'meaning': meaning,
                    'generated_word': word,
                    'logits': logits
                })
        
        # ê²°ê³¼ ì €ì¥
        if results:
            output_file = self.output_dir / f"logits_{self.language}_{self.trial_num}_{self.model_name.replace('/', '-')}.pkl"
            with open(output_file, 'wb') as f:
                pickle.dump(results, f)
            print(f"âœ… Logit ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='ë‹¨ì–´ ìƒì„± logit ë¶„ì„')
    parser.add_argument('--language', '-l', required=True, choices=['en', 'fr', 'ko', 'ja'], help='ì–¸ì–´ ì½”ë“œ')
    parser.add_argument('--model', '-m', required=True, help='ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ')
    parser.add_argument('--trial', '-t', required=True, help='í”„ë¡¬í”„íŠ¸ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸')
    parser.add_argument('--gpu', type=int, default=1, help='ì‚¬ìš©í•  GPU ìˆ˜')
    parser.add_argument('--samples', '-n', type=int, help='ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜')
    
    args = parser.parse_args()
    
    checker = LogitChecker(
        language=args.language,
        model_type='local',
        trial_num=args.trial,
        model_name=args.model,
        tensor_parallel_size=args.gpu
    )
    
    checker.run(args.samples)

if __name__ == "__main__":
    main()
