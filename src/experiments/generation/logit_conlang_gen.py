#!/usr/bin/env python3

# python src/experiments/generation/logit_conlang_gen.py -l en -m Qwen/Qwen3-4B --gpu 4 -t 0.0 --thinking -n 10 -s 1
import os
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd
import pickle
import pprint

from dotenv import load_dotenv
from huggingface_hub import login, model_info
import epitran
from vllm import LLM, SamplingParams
from vllm.distributed import (destroy_distributed_environment, destroy_model_parallel)
from transformers import AutoTokenizer
from openai import OpenAI
from tqdm import tqdm
import torch
import gc
import contextlib

script_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(script_dir, '.env.local')
load_dotenv(dotenv_path=env_path)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')
BASE_DIR = os.getenv('BASE_DIR')

os.environ["HF_HOME"] = os.path.join(script_dir, "../models")
os.environ["TRANSFORMERS_CACHE"] = os.path.join(script_dir, "../models")
os.environ["HF_DATASETS_CACHE"] = os.path.join(script_dir, "../models")
os.environ["HUGGINGFACE_HUB_CACHE"] = os.path.join(script_dir, "../models")

# ëª¨ë¸ ê²½ë¡œ ë§¤í•‘
MODEL_PATHS = {
    "google/gemma-3-27b-it": "google/gemma-3-27b-it",
    "google/gemma-3-12b-it": "google/gemma-3-12b-it",
    "google/gemma-3-4b-it": "google/gemma-3-4b-it",
    "google/gemma-3-1b-it": "google/gemma-3-1b-it",
    "Qwen/Qwen3-4B": "Qwen/Qwen3-4b",
    "Qwen/Qwen3-8B": "Qwen/Qwen3-8b",
    "Qwen/Qwen3-14B": "Qwen/Qwen3-14b",
    "Qwen/Qwen3-32B": "Qwen/Qwen3-32b",
}

language_code = {
    "ko": "kor-Hang",
    "en": "eng-Latn",
    "fr": "fra-Latn",
    "ja": "jpn-Hrgn",
}

HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

def hf_login() -> bool:
    if HUGGINGFACE_TOKEN:
        try:
            login(token=HUGGINGFACE_TOKEN)
            print("âœ… Hugging Faceì— ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âš ï¸ Hugging Face ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
    return False

class LogitConlangGenerator:
    def __init__(
        self,
        model_name: str,
        data_path:str, prompt_path:str, output_dir:str,
        use_api:bool=False,
        samples:int=10, word_nums:int=10, top_k:int=5,
        tensor_parallel_size:int=4,
        max_tokens:int=512, max_model_len:int=4096,
        temperature:float=0.0, thinking:bool=False,
        language:str="ko"
    ):
        
        self.model_name = MODEL_PATHS.get(model_name, model_name)
        data_base_path = data_path
        self.language = language
        self.output_dir = output_dir
        self.prompt_path = prompt_path
        self.use_api = use_api
        self.temperature = temperature
        self.thinking = thinking
        self.tensor_parallel_size = tensor_parallel_size
        self.max_tokens = max_tokens if 'Qwen3' not in model_name else max_model_len
        self.max_model_len = max_model_len
        self.top_k = top_k
        self.word_nums = word_nums
        self.samples = samples
        self.data_path = os.path.join(data_base_path, f"{language}.json")
        
        env_path = Path('.env.local')
        load_dotenv(dotenv_path=env_path)
        
        # vLLM ì„¤ì •
        self.tensor_parallel_size = tensor_parallel_size
        self.max_tokens = max_tokens
        self.max_model_len = max_model_len
        
        if self.use_api:
            self.client = OpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
            )
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.logit_output_dir = Path(f"{BASE_DIR}/sound-symbolism/data/processed/art/logits")
        self.logit_output_dir.mkdir(parents=True, exist_ok=True)

    def _cleanup(self):
        destroy_model_parallel()
        destroy_distributed_environment()
        with contextlib.suppress(AssertionError):
            torch.distributed.destroy_process_group()
        gc.collect()
        torch.cuda.empty_cache()
        
    def load_data(self):
        """ì†ŒìŠ¤ ë°ì´í„°ì™€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        # ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        data_path = Path(f"{BASE_DIR}/sound-symbolism/data/processed/nat/{self.language}.json")
        with open(data_path, 'r', encoding='utf-8') as f:
            self.source_data = json.load(f)
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        prompt_path = Path(f"{BASE_DIR}/sound-symbolism/analysis/experiments/prompts.json")
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompts = json.load(f)
            self.prompt_templates:dict[str, dict[str, str]] = prompts["generation"]

    def load_model(self):
        """vLLM ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        
        # vLLM ëª¨ë¸ ì´ˆê¸°í™”
        self.model = LLM(
            model=self.model_name,
            tensor_parallel_size=self.tensor_parallel_size,
            max_model_len=self.max_model_len,
            trust_remote_code=True
        )
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        self.epi = epitran.Epitran(language_code[self.language])
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def generate_with_logits(self, prompt: str) -> tuple[str, np.ndarray, List[Dict]]:
        """ë‹¨ì–´ ìƒì„± ë° logit íšë“"""
        # ê¸°ë³¸ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            return_logits=True,  # logit ë°˜í™˜ í™œì„±í™”
            logprobs=self.top_k,  # top kê°œì˜ logit ê°’ ë°˜í™˜
            prompt_logprobs=self.top_k,  # í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ logitë„ ë°˜í™˜
            stop=['<end_of_turn>', '</s>', '<|endoftext|>']  # ëª¨ë¸ë³„ stop í† í°
        )
        
        # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì„¤ì •
        if 'Qwen3' in self.model_name:
            conversation = [{"role": "user", "content": prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=self.thinking
            )
        elif 'gemma-3' in self.model_name:
            formatted_prompt = f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n"
        else:
            formatted_prompt = prompt
        
        # í…ìŠ¤íŠ¸ ìƒì„± ë° logit ìˆ˜ì§‘
        outputs = self.model.generate(formatted_prompt, sampling_params)
        generated_text = outputs[0].outputs[0].text.strip()
        
        # logit ì •ë³´ ìˆ˜ì§‘
        logits_list = []
        token_logprobs_list = []
        
        for output in outputs[0].outputs:
            # ì „ì²´ logit í–‰ë ¬ ìˆ˜ì§‘
            if hasattr(output, 'logits'):
                logits_list.append(output.logits)
            
            # í† í°ë³„ ìƒìœ„ kê°œ logit ì •ë³´ ìˆ˜ì§‘
            if hasattr(output, 'logprobs') and output.logprobs:
                token_info = {}
                for token_id, logprob_data in output.logprobs[0].items():
                    token_info[token_id] = {
                        'token': logprob_data.decoded_token,
                        'token_id': token_id,
                        'logprob': logprob_data.logprob,
                        'prob': np.exp(logprob_data.logprob),
                        'rank': len(token_info) + 1  # í† í°ì˜ ìˆœìœ„
                    }
                token_logprobs_list.append(token_info)
        
        # ìƒì„±ëœ ë‹¨ì–´ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        if '`' in generated_text:
            word = generated_text.split('`')[1].strip()
        else:
            # ë°±í‹±ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ ì‹œë„
            word = generated_text.strip()
            # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì „ì²˜ë¦¬ (ì˜ˆ: íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)
        
        # IPA ë³€í™˜ ì‹œë„ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            ipa = self.epi.transliterate(word)
            word_info = {'word': word, 'ipa': ipa}
        except:
            word_info = {'word': word}
        
        # logit í–‰ë ¬ ë³€í™˜ ë° ì •ê·œí™”
        logits_matrix = np.array(logits_list)
        if logits_matrix.size > 0:
            # softmax ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
            logits_matrix = np.exp(logits_matrix) / np.sum(np.exp(logits_matrix), axis=-1, keepdims=True)
        
        return word_info, logits_matrix, token_logprobs_list

    def save_results(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        output_file:os.path = os.path.join(self.output_dir, f"{self.language}_pseudo_words.json")
        csv_file:os.path = os.path.join(self.output_dir, f"{self.language}_pseudo_words.csv")
        
        existing_data = []
        if os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        
        final_results = self.merge_results(existing_data, all_results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        df = pd.DataFrame(final_results)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"âœ… CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ ì™„ë£Œ: {csv_file}")
        
        return final_results
    
    def merge_results(self, existing_data: List[Dict[str, Any]], new_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        result_dict = {}
        for item in existing_data:
            key = f"{item['original_word']}_{item['meaning']}_{item['model']}_{item['trial'][5:]}"
            result_dict[key] = item
        
        updated_count = 0
        added_count = 0
        
        for item in new_data:
            key = f"{item['original_word']}_{item['meaning']}_{item['model']}_{item['trial'][5:]}"
            
            if key in result_dict:
                old_word = result_dict[key].get("generated_word", "")
                result_dict[key] = item
                updated_count += 1
                print(f"ğŸ”„ ë‹¨ì–´ ì—…ë°ì´íŠ¸: '{old_word}' â†’ '{item['generated_word']}'")
            else:
                result_dict[key] = item
                added_count += 1
        
        print(f"ğŸ“Š ê²°ê³¼ ë³‘í•© í†µê³„: {updated_count}ê°œ ì—…ë°ì´íŠ¸, {added_count}ê°œ ì¶”ê°€")
        
        return list(result_dict.values())
    
    def run(self, max_samples: Optional[int] = None):
        """ì „ì²´ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤"""
        if not all([self.load_data(), self.load_model()]):
            return
        
        results = []
        processed = 0
        prompt_keys = self.prompt_templates.keys()
        for key in prompt_keys:
            blank_prompt:str = self.prompt_templates[key][self.language]["user_prompt"]
            for i, item in tqdm(enumerate(self.source_data)):
                if self.word_nums > 0 and i >= self.word_nums:
                    break
                if max_samples and processed >= max_samples:
                    break
                
                num_trials = 0
                word = item["word"]
                
                # ì˜ë¯¸ ì¶”ì¶œ
                if self.language == "ko":
                    definitions:list[str] = item["definitions"]
                    meaning = definitions[0].strip(".")
                else:
                    meaning = item["meaning"].strip(".")
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë‹¨ì–´ ìƒì„±
                prompt = blank_prompt.format(meaning=meaning)
                word_info, logits, token_logprobs = self.generate_with_logits(prompt)
                
                if len(word_info['word']) > 0 and logits.size > 0:
                    results.append({
                        "original_word": item["word"],
                        "meaning": item["meaning"],
                        "generated_word": word_info["word"],
                        "ipa": word_info["ipa"],
                        "logits_matrix": logits,  # ì „ì²´ logit í–‰ë ¬
                        "token_logprobs": token_logprobs,  # í† í°ë³„ ìƒìœ„ kê°œ logit ê°’
                        "model": self.model_name,
                        "trial": key
                    })
                    processed += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if processed % 10 == 0:
                    gc.collect()
                    torch.cuda.empty_cache()
        
        # ê²°ê³¼ ì €ì¥
        if results:
            output_file = self.output_dir / f"{self.language}_logit_{self.model_name.replace('/', '-')}.pkl"
            with open(output_file, 'wb') as f:
                pickle.dump(results, f)
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='Conlang ìƒì„± ë° Logit ë¶„ì„')
    parser.add_argument('--language', '-l', required=True, choices=['en', 'fr', 'ko', 'ja'], help='ì–¸ì–´ ì½”ë“œ')
    parser.add_argument('--model', '-m', required=True, help='ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--gpu', type=int, default=4, help='ì‚¬ìš©í•  GPU ìˆ˜')
    parser.add_argument("--max-tokens", type=int, default=512, help="Maximum tokens to generate")
    parser.add_argument("--max-model-len", type=int, default=4096, help="Maximum model length")
    parser.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature")
    parser.add_argument("--api", action='store_true', help="Use OpenAI API instead of local model")
    parser.add_argument("--thinking", '-t', action='store_true', help="Enable thinking mode for Qwen3")
    parser.add_argument("--word_nums", '-n', type=int, default=10, help="Number of words to generate")
    parser.add_argument("--samples", '-s', type=int, default=1, help="Number of samples to generate")
    parser.add_argument("--top_k", '-k', type=int, default=3, help="Number of top k logits to return")
    args = parser.parse_args()
    
    generator = LogitConlangGenerator(
        language=args.language,
        model_name=args.model,
        tensor_parallel_size=args.gpu,
        max_tokens=args.max_tokens,
        max_model_len=args.max_model_len,
        temperature=args.temperature,
        thinking=args.thinking,
        word_nums=args.word_nums,
        samples=args.samples,
        top_k=args.top_k
    )
    
    generator.run(args.samples)

if __name__ == "__main__":
    main()
