#!/usr/bin/env python3
# python pseudo_word_generation.py -l ko -m openai --model-name gpt-4o -t trial2 -n 5 -w 2
# python pseudo_word_generation.py -l ko -m local --model-name gemma-3-27b-it -t trial2 -n 100 -w 2
# python pseudo_word_generation.py -l ko -m local --local-model qwen3-4b -t trial10 
# python pseudo_word_generation.py --download-model bloom-560m
# python pseudo_word_generation.py --debug-model gpt2
# python pseudo_word_generation.py -l ko -m openai --model-name gpt-4o --all-trials -n 10
# python pseudo_word_generation.py -l ko -m local --model-name qwen3-14b --all-trials -n 100

import os
import json
import argparse
import time
import traceback
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import pandas as pd
from typing import List, Dict, Any, Optional
from huggingface_hub import login, model_info
import shutil
from openai import OpenAI, AsyncOpenAI
import psutil
import sys

# HuggingFace ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')
HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ì˜ - Hugging Face ëª¨ë¸ IDë§Œ í¬í•¨
AVAILABLE_MODELS = {
    # Gemma ëª¨ë¸
    "gemma-3-4b-it": {
        "hf_id": "google/gemma-3-4b-it",
        "description": "Googleì˜ Gemma 3 4B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-12b-it": {
        "hf_id": "google/gemma-3-12b-it", 
        "description": "Googleì˜ Gemma 3 12B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-27b-it": {
        "hf_id": "google/gemma-3-27b-it",
        "description": "Googleì˜ Gemma 3 27B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    
    # Qwen 2.5 ëª¨ë¸
    "qwen2.5-3b": {
        "hf_id": "Qwen/Qwen2.5-3B-Instruct",
        "description": "Qwenì˜ 2.5 3B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-7b": {
        "hf_id": "Qwen/Qwen2.5-7B-Instruct",
        "description": "Qwenì˜ 2.5 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-14b": {
        "hf_id": "Qwen/Qwen2.5-14B-Instruct",
        "description": "Qwenì˜ 2.5 14B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-32b": {
        "hf_id": "Qwen/Qwen2.5-32B-Instruct",
        "description": "Qwenì˜ 2.5 32B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-72b": {
        "hf_id": "Qwen/Qwen2.5-72B-Instruct",
        "description": "Qwenì˜ 2.5 72B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 ëª¨ë¸
    "qwen3-4b": {
        "hf_id": "Qwen/Qwen3-4B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b": {
        "hf_id": "Qwen/Qwen3-8B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b": {
        "hf_id": "Qwen/Qwen3-14B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b": {
        "hf_id": "Qwen/Qwen3-32B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 Thinking ëª¨ë¸
    "qwen3-4b-thinking": {
        "hf_id": "Qwen/Qwen3-4B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b-thinking": {
        "hf_id": "Qwen/Qwen3-8B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b-thinking": {
        "hf_id": "Qwen/Qwen3-14B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b-thinking": {
        "hf_id": "Qwen/Qwen3-32B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    
    # OLMo ëª¨ë¸
    "olmo-7b": {
        "hf_id": "allenai/OLMo-2-1124-7B-Instruct",
        "description": "Allen AIì˜ OLMo 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-13b": {
        "hf_id": "allenai/OLMo-2-1124-13B-Instruct",
        "description": "Allen AIì˜ OLMo 13B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-32b": {
        "hf_id": "allenai/OLMo-2-0325-32B-Instruct",
        "description": "Allen AIì˜ OLMo 32B Instruct ëª¨ë¸",
        "requires_auth": False
    }
}

# ì‚¬ìš© ê°€ëŠ¥í•œ OpenAI ëª¨ë¸ ëª©ë¡
OPENAI_MODELS = [
    "gpt-4o",
    "gpt-4-turbo", 
    "gpt-4",
    "gpt-4.1",
    "gpt-3.5-turbo"
]

# Hugging Face ë¡œê·¸ì¸ í•¨ìˆ˜
def hf_login() -> bool:
    if HUGGINGFACE_TOKEN:
        try:
            login(token=HUGGINGFACE_TOKEN)
            print("âœ… Hugging Faceì— ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âš ï¸ Hugging Face ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
    return False

# ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def check_model_access(model_name: str) -> bool:
    if model_name not in AVAILABLE_MODELS:
        print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")
        return False
    
    model_id = AVAILABLE_MODELS[model_name]["hf_id"]
    print(f"ğŸ” ëª¨ë¸ '{model_id}' ì ‘ê·¼ì„± í™•ì¸ ì¤‘...")
    try:
        info = model_info(model_id)
        return True
    except Exception as e:
        print(f"âŒ ëª¨ë¸ '{model_id}'ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False

# ëª¨ë¸ ë””ë²„ê¹… í•¨ìˆ˜
def debug_model(model_name: str) -> None:
    if model_name not in AVAILABLE_MODELS:
        print(f"âŒ ëª¨ë¸ '{model_name}'ì€(ëŠ”) AVAILABLE_MODELSì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    model_info = AVAILABLE_MODELS[model_name]
    model_id = model_info["hf_id"]
    
    print(f"===== ëª¨ë¸ ë””ë²„ê¹… =====")
    print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
    print(f"Hugging Face ID: {model_id}")
    print(f"ì„¤ëª…: {model_info['description']}")
    
    # ë¡œê·¸ì¸ ì‹œë„
    hf_login()
    
    # ëª¨ë¸ ì ‘ê·¼ì„± í™•ì¸
    try:
        print("\nğŸ” ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘...")
        info = model_info(model_id)
        print(f"âœ… ëª¨ë¸ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print(f"ëª¨ë¸ íƒ€ì…: {info.modelId}")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        pipe = pipeline("text-generation", model=model_id, max_new_tokens=20)
        test_result = pipe("This is a test:")
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_result[0]['generated_text']}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    print("===== ë””ë²„ê¹… ì™„ë£Œ =====")

# ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì • í•¨ìˆ˜ ì¶”ê°€
def setup_model_cache():
    """HuggingFace ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •"""
    # ì‚¬ìš©ì ì§€ì • ìºì‹œ ë””ë ‰í† ë¦¬
    custom_cache_dir = "/scratch2/sheepswool/workspace/models"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(custom_cache_dir):
        try:
            os.makedirs(custom_cache_dir, exist_ok=True)
            print(f"âœ… ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {custom_cache_dir}")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["HUGGINGFACE_HUB_CACHE"] = custom_cache_dir
    os.environ["TRANSFORMERS_CACHE"] = custom_cache_dir
    os.environ["HF_HOME"] = custom_cache_dir
    
    # íŒŒì¼ ê¶Œí•œ í™•ì¸
    try:
        test_file = os.path.join(custom_cache_dir, "test_write.txt")
        with open(test_file, 'w') as f:
            f.write("Test write permission")
        os.remove(test_file)
        return True
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ ë˜ëŠ” ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

class PseudoWordGenerator:
    """ê°€ìƒ ë‹¨ì–´ ìƒì„±ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, language: str, model: str, trial_num: str, 
                 batch_size: int = 10, output_dir: Optional[str] = None, 
                 local_model: Optional[str] = None):
        """
        ê°€ìƒ ë‹¨ì–´ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            language: ì–¸ì–´ ì½”ë“œ (en/fr/ko/ja)
            model: ëª¨ë¸ íƒ€ì… (openai, local)
            trial_num: í”„ë¡¬í”„íŠ¸ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ (trial1, trial2, ...)
            batch_size: ë°°ì¹˜ í¬ê¸°
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            local_model: ë¡œì»¬ ëª¨ë¸ ì´ë¦„
        """
        self.language = language.lower()
        self.model_type = model.lower()
        self.trial_num = trial_num
        self.batch_size = batch_size
        self.local_model_name = local_model
        self.model_name = "gpt-4o"  # OpenAI ê¸°ë³¸ ëª¨ë¸
        self.words_per_meaning = 1
        
        # ë¡œì»¬ ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.client = None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = Path(f"../../dataset/0_raw/art")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.user_prompt_template = None
        
        # ì†ŒìŠ¤ ë°ì´í„°
        self.source_data = []
    
    def set_trial_num(self, trial_num: str):
        """
        íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ ë³€ê²½
        
        Args:
            trial_num: ìƒˆë¡œìš´ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸
        """
        self.trial_num = trial_num
        # í”„ë¡¬í”„íŠ¸ë„ ë‹¤ì‹œ ë¡œë“œ
        self.user_prompt_template = None
        self.load_prompts()
    
    def load_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        try:
            prompts_path = Path("../../analysis/experiments/prompts.json")

            with open(prompts_path, 'r', encoding='utf-8') as f:
                prompts_data = json.load(f)
            
            trial_data = prompts_data["generation"][self.trial_num]
            self.user_prompt_template = trial_data[self.language]["user_prompt"]
            print(f"âœ… í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ (íŠ¸ë¼ì´ì–¼: {self.trial_num}, ì–¸ì–´: {self.language})")
            return True
            
        except Exception as e:
            traceback.print_exc()
            return False
    
    def load_source_data(self):
        """ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ"""
        # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        file_path = Path(f"../../dataset/1_preprocess/nat/{self.language}.json")
        with open(file_path, 'r', encoding='utf-8') as f:
            self.source_data = json.load(f)
        
        print(f"âœ… ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(self.source_data)}ê°œ í•­ëª©)")
        return True
    
    def load_local_model(self):
        """ë¡œì»¬ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not self.local_model_name:
                print("âŒ ë¡œì»¬ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            model_config = AVAILABLE_MODELS[self.local_model_name]
            model_id = model_config["hf_id"]
            requires_auth = model_config["requires_auth"]
            
            # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
            cache_dir = "/scratch2/sheepswool/workspace/models"
            
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            print(f"ğŸ“‚ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
            
            if requires_auth and HUGGINGFACE_TOKEN:
                login(token=HUGGINGFACE_TOKEN)
                print(f"âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ")
            
            # 4ë¹„íŠ¸ ì–‘ìí™” ì ìš©
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_id,
                    cache_dir=cache_dir
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    cache_dir=cache_dir,
                    load_in_4bit=True,
                    device_map="auto"
                )
                print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (4ë¹„íŠ¸ ì–‘ìí™”)")
                return True
                
            except Exception as e:
                print(f"âš ï¸ 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ” ì¼ë°˜ ëª¨ë“œë¡œ ë‹¤ì‹œ ì‹œë„...")
                
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_id,
                        cache_dir=cache_dir
                    )
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_id,
                        cache_dir=cache_dir,
                        device_map="auto"
                    )
                    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¼ë°˜ ëª¨ë“œ)")
                    return True
                except Exception as e:
                    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return False
    
    def get_local_model_completion(self, prompt: str) -> str:
        """ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        # íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸)
        if self.pipeline:
            outputs = self.pipeline(
                prompt,
                max_new_tokens=50,
                do_sample=True,
                temperature=1.0,
                top_p=0.9,
                num_return_sequences=1
            )
            return outputs[0]["generated_text"][len(prompt):]
        else:
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì§ì ‘ ì‚¬ìš©
            inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=1.0,
                top_p=0.9,
                num_return_sequences=1
            )
            return self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    def get_openai_completion(self, prompt: str) -> str:
        if not self.client:
            self.client = OpenAI(api_key=OPENAI_API_KEY)
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=1.0,
            max_tokens=50
        )
        # answer = response.choices[0].message.content.strip()
        # breakpoint()
        return response.choices[0].message.content.strip()
    
    def generate_multiple_words(self, meaning: str, count: int) -> List[str]:
        words = []
        
        for i in range(count):
            prompt = self.user_prompt_template.format(meaning=meaning)
            
            if self.model_type == "openai":
                result = self.get_openai_completion(prompt)
            else:
                result = self.get_local_model_completion(prompt)
            
            if result:
                word = self.extract_generated_word(result)
                words.append(word)
                print(f"  [ë‹¨ì–´ {i+1}/{count}] '{word}'")
            
            if self.model_type == "openai" and i < count - 1:
                time.sleep(0.5)
        
        return words
    
    def extract_generated_word(self, text: str) -> str:
        import re
        backtick_pattern = re.compile(r'`([^`]+)`')
        backtick_matches = backtick_pattern.findall(text)
        
        if backtick_matches:
            return backtick_matches[0].strip()
        
        bracket_pattern = re.compile(r'\[([^\]]+)\]')
        bracket_matches = bracket_pattern.findall(text)
        
        if bracket_matches:
            print(f"ğŸ” {bracket_matches}")
            return bracket_matches[0].strip()
        
        text = text.strip()
        if len(text) > 20:
            return text[:20] + "..."
        
        return text
    
    def run(self, max_words=None):
        # ì˜ë¯¸ ëª©ë¡ ì¤€ë¹„
        self.load_source_data()
        self.load_prompts()
        meanings = [item.get('definitions', '') for item in self.source_data]
        original_words = [item.get('word', '') for item in self.source_data]
        
        # ìµœëŒ€ ì²˜ë¦¬í•  ì˜ë¯¸ ìˆ˜ ì œí•œ
        if max_words is not None and max_words > 0:
            meanings = meanings[:max_words]
        
        print(f"ğŸ¯ ì´ {len(meanings)}ê°œì˜ ì˜ë¯¸ì— ëŒ€í•´ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        print(f"ğŸ” ì–¸ì–´: {self.language}, ëª¨ë¸: {self.model_type}, íŠ¸ë¼ì´ì–¼: {self.trial_num}")
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if self.model_type == "openai":
            self.client = OpenAI(api_key=OPENAI_API_KEY)
            print(f"âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model_name})")
        else:
            # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
            if not self.load_local_model():
                print("âŒ ë¡œì»¬ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
        
        all_results = []
        if self.words_per_meaning == 1:
            # ë°°ì¹˜ ì²˜ë¦¬ (ì˜ë¯¸ë‹¹ ë‹¨ì–´ 1ê°œ)
            batch_meanings = [meanings[i:i+self.batch_size] for i in range(0, len(meanings), self.batch_size)]
            
            for batch_idx, batch in enumerate(batch_meanings):
                print(f"ğŸ”„ ë°°ì¹˜ {batch_idx+1}/{len(batch_meanings)} ì²˜ë¦¬ ì¤‘...")
                
                # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompts = [self.user_prompt_template.format(meaning=meaning[0]) for meaning in batch]
                
                # ë°°ì¹˜ ì²˜ë¦¬ (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼)
                if self.model_type == "openai":
                    results = []
                    for prompt in tqdm(prompts, desc="OpenAI API í˜¸ì¶œ"):
                        result = self.get_openai_completion(prompt)
                        results.append(result)
                        time.sleep(0.5)  # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                else:
                    results = []
                    for prompt in tqdm(prompts, desc="ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ"):
                        result = self.get_local_model_completion(prompt)
                        results.append(result)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for i, (meaning, result) in enumerate(zip(batch, results)):
                    if result:
                        generated_word = self.extract_generated_word(result)
                        
                        item_result = {
                            "original_word": original_words[self.batch_size*batch_idx+i],
                            "meaning": meaning[0],
                            "generated_word": generated_word,
                            "model": self.model_name if self.model_type == "openai" else self.local_model_name,
                            "language": self.language,
                            "trial": self.trial_num
                        }
                        
                        all_results.append(item_result)
                        print(f"  [{(batch_idx*self.batch_size)+i+1}/{len(meanings)}] ì˜ë¯¸: '{meaning[:30]}...' â†’ ë‹¨ì–´: '{generated_word}'")
                        
        else:
            for i, meaning in enumerate(meanings):
                print(f"ğŸ”„ ì˜ë¯¸ {i+1}/{len(meanings)} ì²˜ë¦¬ ì¤‘: '{meaning[:30]}...'")
                
                words = self.generate_multiple_words(meaning[0], self.words_per_meaning)
                
                for j, word in enumerate(words):
                    item_result = {
                        "original_word": original_words[i],
                        "meaning": meaning[0],
                        "generated_word": word,
                        "model": self.model_name if self.model_type == "openai" else self.local_model_name,
                        "language": self.language,
                        "trial": self.trial_num
                    }
                    
                    all_results.append(item_result)
        
        final_results = self.save_results(all_results)
        return final_results
    
    def save_results(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        output_file = self.output_dir / f"pseudo_words_{self.language}.json"
        csv_file = self.output_dir / f"pseudo_words_{self.language}.csv"
        
        existing_data = []
        if output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                print(f"âœ… ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {output_file} ({len(existing_data)}ê°œ í•­ëª©)")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤: {e}")
        
        final_results = self.merge_results(existing_data, all_results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file} (ì´ {len(final_results)}ê°œ í•­ëª©)")
        
        df = pd.DataFrame(final_results)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"âœ… CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ ì™„ë£Œ: {csv_file}")
        
        return final_results
    
    def merge_results(self, existing_data: List[Dict[str, Any]], new_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        result_dict = {}
        for item in existing_data:
            key = f"{item['original_word']}_{item['meaning']}_{item['model']}_{item['trial'][5:]}"
            result_dict[key] = item
        
        updated_count = 0
        added_count = 0
        
        for item in new_data:
            key = f"{item['original_word']}_{item['meaning']}_{item['model']}_{item['trial'][5:]}"
            
            if key in result_dict:
                old_word = result_dict[key].get("generated_word", "")
                result_dict[key] = item
                updated_count += 1
                print(f"ğŸ”„ ë‹¨ì–´ ì—…ë°ì´íŠ¸: '{old_word}' â†’ '{item['generated_word']}'")
            else:
                result_dict[key] = item
                added_count += 1
        
        print(f"ğŸ“Š ê²°ê³¼ ë³‘í•© í†µê³„: {updated_count}ê°œ ì—…ë°ì´íŠ¸, {added_count}ê°œ ì¶”ê°€")
        
        return list(result_dict.values())

def setup_model_cache():
    os.environ["TRANSFORMERS_CACHE"] = "/scratch2/sheepswool/model_cache"
    os.environ["HF_HOME"] = "/scratch2/sheepswool/model_cache"

def setup_requirements():
    try:
        import torch
        import transformers
        print("âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (torch, transformers)")
    except ImportError:
        print("âš ï¸ ê¸°ë³¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        import subprocess
        subprocess.check_call(["pip", "install", "torch", "transformers"])
    
    # ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import bitsandbytes
        import accelerate
        print("âœ… ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (bitsandbytes, accelerate)")
    except ImportError:
        print("âš ï¸ ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤ (ëŒ€í˜• ëª¨ë¸ì— í•„ìš”)...")
        import subprocess
        subprocess.check_call(["pip", "install", "bitsandbytes", "accelerate"])
    
    print("âœ… ëª¨ë“  í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
    setup_model_cache()
    
    parser = argparse.ArgumentParser(description='ê°€ìƒ ë‹¨ì–´ ìƒì„± ë„êµ¬')
    parser.add_argument('--language', '-l', choices=['en', 'fr', 'ko', 'ja'], 
                        help='ì–¸ì–´ ì½”ë“œ (en/fr/ko/ja)')
    parser.add_argument('--model', '-m', default='local', choices=['local', 'openai'],
                        help='ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (local/openai)')
    
    # íŠ¸ë¼ì´ì–¼ ì˜µì…˜ ê·¸ë£¹
    trial_group = parser.add_mutually_exclusive_group()
    trial_group.add_argument('--trial', '-t', help='prompts.jsonì˜ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ (ì˜ˆ: trial1, trial2)')
    trial_group.add_argument('--all-trials', action='store_true', 
                             help='ëª¨ë“  íŠ¸ë¼ì´ì–¼ ì‹¤í–‰ (trial1~trial11)')
    trial_group.add_argument('--trial-range', type=str, 
                             help='íŠ¸ë¼ì´ì–¼ ë²”ìœ„ ì§€ì • (ì˜ˆ: 1-5, 7,9,11)')
    
    parser.add_argument('--batch-size', '-b', type=int, default=10,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--num-words', '-n', type=int, default=None,
                        help='ì²˜ë¦¬í•  ì˜ë¯¸ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)')
    parser.add_argument('--words-per-meaning', '-w', type=int, default=1,
                        help='ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 1)')
    parser.add_argument('--output-dir', '-o', type=str, default=None,
                        help='ìƒì„±ëœ ë‹¨ì–´ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--model-name', type=str, default=None,
                        help='ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (OpenAI ë˜ëŠ” ë¡œì»¬ ëª¨ë¸)')
    parser.add_argument('--debug-model', type=str, choices=list(AVAILABLE_MODELS.keys()),
                        help='ëª¨ë¸ ë””ë²„ê¹…')
    parser.add_argument('--all-models', action='store_true',
                        help='ëª¨ë“  ë¡œì»¬ ëª¨ë¸ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë””ë²„ê¹…
    if args.debug_model:
        debug_model(args.debug_model)
        exit(0)
    
    # í•„ìˆ˜ ì¸ì í™•ì¸
    if not args.language:
        parser.error("--language ì¸ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    if not (args.trial or args.all_trials or args.trial_range):
        parser.error("--trial, --all-trials, ë˜ëŠ” --trial-range ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œ --all-models ì˜µì…˜ ë¬´ì‹œ
    if args.model == 'openai' and args.all_models:
        print("âš ï¸ OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œ --all-models ì˜µì…˜ì€ ë¬´ì‹œë©ë‹ˆë‹¤.")
        args.all_models = False
    
    # íŠ¸ë¼ì´ì–¼ ëª©ë¡ ê²°ì •
    trials_to_run = []
    
    if args.all_trials:
        # ëª¨ë“  íŠ¸ë¼ì´ì–¼ ì‹¤í–‰ (trial1~trial11)
        trials_to_run = [f"trial{i}" for i in range(1, 12)]
    elif args.trial_range:
        # ë²”ìœ„ íŒŒì‹±
        ranges = args.trial_range.split(',')
        for r in ranges:
            if '-' in r:
                start, end = map(int, r.split('-'))
                trials_to_run.extend([f"trial{i}" for i in range(start, end+1)])
            else:
                trials_to_run.append(f"trial{int(r)}")
    else:
        # ë‹¨ì¼ íŠ¸ë¼ì´ì–¼
        trials_to_run = [args.trial]
    
    # ëª¨ë¸ ëª©ë¡ ê²°ì •
    models_to_run = []
    
    if args.all_models:
        # ëª¨ë“  ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰
        models_to_run = list(AVAILABLE_MODELS.keys())
        print(f"ğŸ”„ ëª¨ë“  ë¡œì»¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤: {len(models_to_run)}ê°œ")
    elif args.model == 'local' and args.model_name:
        # íŠ¹ì • ë¡œì»¬ ëª¨ë¸ë§Œ ì‹¤í–‰
        models_to_run = [args.model_name]
    elif args.model == 'local':
        # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        models_to_run = ["gpt2"]
        print(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ {models_to_run[0]}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    setup_requirements()
    
    # OpenAI ëª¨ë¸ ì‚¬ìš©
    if args.model == 'openai':
        # ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = PseudoWordGenerator(
            language=args.language,
            model=args.model,
            trial_num=trials_to_run[0],  # ì²« ë²ˆì§¸ íŠ¸ë¼ì´ì–¼ë¡œ ì´ˆê¸°í™”
            batch_size=args.batch_size,
            output_dir=args.output_dir,
            local_model=None
        )
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì •
        if args.model_name:
            generator.model_name = args.model_name
        
        # ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ ì„¤ì •
        if args.words_per_meaning > 1:
            generator.set_words_per_meaning(args.words_per_meaning)
        
        # ëª¨ë“  íŠ¸ë¼ì´ì–¼ ì‹¤í–‰
        for trial in trials_to_run:
            print(f"\n{'='*80}")
            print(f"ğŸ” íŠ¸ë¼ì´ì–¼ {trial} ì‹¤í–‰ ì¤‘... (ëª¨ë¸: {generator.model_name})")
            print(f"{'='*80}\n")
            
            # íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ ì„¤ì •
            generator.set_trial_num(trial)
            
            # ì‹¤í–‰
            generator.run(args.num_words)
    else:
        # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
        # ëª¨ë“  ë¡œì»¬ ëª¨ë¸ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        for model_name in models_to_run:
            print(f"\n{'#'*100}")
            print(f"ğŸ” ëª¨ë¸ {model_name} ë¡œë“œ ì¤‘...")
            print(f"{'#'*100}\n")
            
            try:
                # ìƒì„±ê¸° ì´ˆê¸°í™”
                generator = PseudoWordGenerator(
                    language=args.language,
                    model='local',
                    trial_num=trials_to_run[0],  # ì²« ë²ˆì§¸ íŠ¸ë¼ì´ì–¼ë¡œ ì´ˆê¸°í™”
                    batch_size=args.batch_size,
                    output_dir=args.output_dir,
                    local_model=model_name
                )
                
                # ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ ì„¤ì •
                if args.words_per_meaning > 1:
                    generator.set_words_per_meaning(args.words_per_meaning)
                
                # ëª¨ë¸ ë¡œë“œ ì‹œë„
                success = generator.load_local_model()
                
                if not success:
                    print(f"âŒ ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    continue
                
                # ëª¨ë“  íŠ¸ë¼ì´ì–¼ ì‹¤í–‰
                for trial in trials_to_run:
                    print(f"\n{'='*80}")
                    print(f"ğŸ” íŠ¸ë¼ì´ì–¼ {trial} ì‹¤í–‰ ì¤‘... (ëª¨ë¸: {model_name})")
                    print(f"{'='*80}\n")
                    
                    # íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ ì„¤ì •
                    generator.set_trial_num(trial)
                    
                    # ì‹¤í–‰
                    try:
                        generator.run(args.num_words)
                    except Exception as e:
                        print(f"âŒ íŠ¸ë¼ì´ì–¼ {trial} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        traceback.print_exc()
                        print("âš ï¸ ë‹¤ìŒ íŠ¸ë¼ì´ì–¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                        continue
            
            except Exception as e:
                print(f"âŒ ëª¨ë¸ {model_name} ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()
                print("âš ï¸ ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                continue
            
            finally:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if 'generator' in locals() and generator.model:
                    try:
                        del generator.model
                        del generator.tokenizer
                        del generator.pipeline
                        import gc
                        gc.collect()
                        
                        if 'torch' in sys.modules:
                            import torch
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                        
                        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                    except:
                        pass
    
    print("\nï¿½ï¿½ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
