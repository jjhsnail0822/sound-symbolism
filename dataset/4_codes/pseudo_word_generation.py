#!/usr/bin/env python3
# python pseudo_word_generation.py -l ko -m local --local-model gpt2 -t trial10 -n 5 -w 2
# python pseudo_word_generation.py -l ko -m local --local-model gemma-3-12b-it -t trial2 -n 100 -w 2
# python pseudo_word_generation.py -l ko -m local --local-model qwen3-4b -t trial10 
# python pseudo_word_generation.py --download-model bloom-560m
# python pseudo_word_generation.py --debug-model gpt2

import os
import json
import argparse
import random
import time
import traceback
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import pandas as pd
from typing import List, Dict, Any, Optional
from huggingface_hub import login, model_info

# HuggingFace ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import pipeline, AutoTokenizer

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')
HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ì˜ - Hugging Face ëª¨ë¸ IDë§Œ í¬í•¨
AVAILABLE_MODELS = {
    # Gemma ëª¨ë¸
    "gemma-3-4b-it": {
        "hf_id": "google/gemma-3-4b-it",
        "description": "Googleì˜ Gemma 3 4B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-12b-it": {
        "hf_id": "google/gemma-3-12b-it", 
        "description": "Googleì˜ Gemma 3 12B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-27b-it": {
        "hf_id": "google/gemma-3-27b-it",
        "description": "Googleì˜ Gemma 3 27B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    
    # Qwen 2.5 ëª¨ë¸
    "qwen2.5-3b": {
        "hf_id": "Qwen/Qwen2.5-3B-Instruct",
        "description": "Qwenì˜ 2.5 3B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-7b": {
        "hf_id": "Qwen/Qwen2.5-7B-Instruct",
        "description": "Qwenì˜ 2.5 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-14b": {
        "hf_id": "Qwen/Qwen2.5-14B-Instruct",
        "description": "Qwenì˜ 2.5 14B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-32b": {
        "hf_id": "Qwen/Qwen2.5-32B-Instruct",
        "description": "Qwenì˜ 2.5 32B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-72b": {
        "hf_id": "Qwen/Qwen2.5-72B-Instruct",
        "description": "Qwenì˜ 2.5 72B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 ëª¨ë¸
    "qwen3-4b": {
        "hf_id": "Qwen/Qwen3-4B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b": {
        "hf_id": "Qwen/Qwen3-8B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b": {
        "hf_id": "Qwen/Qwen3-14B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b": {
        "hf_id": "Qwen/Qwen3-32B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 Thinking ëª¨ë¸
    "qwen3-4b-thinking": {
        "hf_id": "Qwen/Qwen3-4B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b-thinking": {
        "hf_id": "Qwen/Qwen3-8B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b-thinking": {
        "hf_id": "Qwen/Qwen3-14B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b-thinking": {
        "hf_id": "Qwen/Qwen3-32B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    
    # OLMo ëª¨ë¸
    "olmo-7b": {
        "hf_id": "allenai/OLMo-2-1124-7B-Instruct",
        "description": "Allen AIì˜ OLMo 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-13b": {
        "hf_id": "allenai/OLMo-2-1124-13B-Instruct",
        "description": "Allen AIì˜ OLMo 13B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-32b": {
        "hf_id": "allenai/OLMo-2-0325-32B-Instruct",
        "description": "Allen AIì˜ OLMo 32B Instruct ëª¨ë¸",
        "requires_auth": False
    }
}

# Hugging Face ë¡œê·¸ì¸ í•¨ìˆ˜
def hf_login() -> bool:
    """Hugging Faceì— ë¡œê·¸ì¸"""
    if HUGGINGFACE_TOKEN:
        try:
            login(token=HUGGINGFACE_TOKEN)
            print("âœ… Hugging Faceì— ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âš ï¸ Hugging Face ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì¼ë¶€ ëª¨ë¸ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return False

# ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def check_model_access(model_name: str) -> bool:
    """ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    if model_name not in AVAILABLE_MODELS:
        print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(AVAILABLE_MODELS.keys())}")
        return False
    
    model_id = AVAILABLE_MODELS[model_name]["hf_id"]
    print(f"ğŸ” ëª¨ë¸ '{model_id}' ì ‘ê·¼ì„± í™•ì¸ ì¤‘...")
    
    try:
        info = model_info(model_id)
        print(f"âœ… ëª¨ë¸ '{model_id}'ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"âŒ ëª¨ë¸ '{model_id}'ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ì´ ëª¨ë¸ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ ì ‘ê·¼ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False

# ëª¨ë¸ ë””ë²„ê¹… í•¨ìˆ˜
def debug_model(model_name: str) -> None:
    """ëª¨ë¸ ì ‘ê·¼ì„± ë””ë²„ê¹…"""
    if model_name not in AVAILABLE_MODELS:
        print(f"âŒ ëª¨ë¸ '{model_name}'ì€(ëŠ”) AVAILABLE_MODELSì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    model_info = AVAILABLE_MODELS[model_name]
    model_id = model_info["hf_id"]
    
    print(f"===== ëª¨ë¸ ë””ë²„ê¹… =====")
    print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
    print(f"Hugging Face ID: {model_id}")
    print(f"ì„¤ëª…: {model_info['description']}")
    
    # ë¡œê·¸ì¸ ì‹œë„
    hf_login()
    
    # ëª¨ë¸ ì ‘ê·¼ì„± í™•ì¸
    try:
        print("\nğŸ” ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘...")
        info = model_info(model_id)
        print(f"âœ… ëª¨ë¸ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print(f"ëª¨ë¸ íƒ€ì…: {info.modelId}")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        try:
            pipe = pipeline("text-generation", model=model_id, max_new_tokens=20)
            test_result = pipe("This is a test:")
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_result[0]['generated_text']}")
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    print("===== ë””ë²„ê¹… ì™„ë£Œ =====")

class PseudoWordGenerator:
    def __init__(self, language: str, model: str = "local", trial_num: str = "trial1", 
                 batch_size: int = 10, output_dir: Optional[str] = None, 
                 local_model: Optional[str] = "gpt2"):
        """ê°€ìƒ ë‹¨ì–´ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.language = language.lower()
        if self.language not in ['en', 'ja', 'ko', 'fr']:
            raise ValueError("ì–¸ì–´ëŠ” 'en', 'ja', 'ko', 'fr' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        self.model_type = model.lower()
        if self.model_type != "local":
            self.model_type = "local"  # OpenAI ëŒ€ì‹  ë¬´ì¡°ê±´ local ëª¨ë¸ ì‚¬ìš©
            print("âš ï¸ OpenAI API ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        self.trial_num = trial_num
        self.batch_size = batch_size
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = Path(f"../0_raw/art/{self.language}")
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œì»¬ ëª¨ë¸ ì„¤ì •
        self.local_model_name = local_model or "gpt2"
        if self.local_model_name not in AVAILABLE_MODELS:
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {self.local_model_name}")
            print(f"gpt2 ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.local_model_name = "gpt2"
        
        # ëª¨ë¸ íŒŒì´í”„ë¼ì¸ (ì²˜ìŒì—ëŠ” None, í•„ìš”í•  ë•Œ ë¡œë“œ)
        self.pipeline = None
        
        # í”„ë¡¬í”„íŠ¸ ë° ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        self.load_prompts()
        self.load_source_data()
        
        # ìƒì„± ì„¤ì •
        self.words_per_meaning = 1  # ê¸°ë³¸ê°’: ì˜ë¯¸ë‹¹ 1ê°œ ë‹¨ì–´
    
    def load_prompts(self) -> None:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        # prompts.json íŒŒì¼ ê²½ë¡œ
        prompts_file = Path("../../analysis/experiments/prompts.json")
        
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts_data = json.load(f)
        
        # generation í‚¤ ì•„ë˜ì˜ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
        generation_prompts = prompts_data.get('generation', {})
        
        # íŠ¸ë¼ì´ì–¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if self.trial_num not in generation_prompts:
            print(f"âŒ generation í”„ë¡¬í”„íŠ¸ì—ì„œ {self.trial_num}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            available_trials = list(generation_prompts.keys())
            if available_trials:
                self.trial_num = available_trials[0]
                print(f"ëŒ€ì‹  {self.trial_num}ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¸ë¼ì´ì–¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŠ¸ë¼ì´ì–¼ ì •ë³´ ë¡œë“œ
        trial_info = generation_prompts[self.trial_num]
        
        # ì„¤ëª… ì¶œë ¥ (ìˆìœ¼ë©´)
        if 'explanation' in trial_info:
            print(f"\n===== Trial {self.trial_num} ì •ë³´ =====")
            for key, value in trial_info['explanation'].items():
                print(f"  {key}: {value}")
        
        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        if self.language in trial_info:
            if 'user_prompt' in trial_info[self.language] and trial_info[self.language]['user_prompt']:
                # í”„ë¡¬í”„íŠ¸ ìµœì í™”
                optimized_prompt = self.optimize_prompt(trial_info)
                if optimized_prompt:
                    self.prompt_template = optimized_prompt
                    print(f"âœ… {self.language} ì–¸ì–´ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ì—¬ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                else:
                    self.prompt_template = trial_info[self.language]['user_prompt']
                    print(f"âœ… {self.language} ì–¸ì–´ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            else:
                # ì„ íƒëœ ì–¸ì–´ì— í”„ë¡¬í”„íŠ¸ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ
                print(f"âš ï¸ {self.trial_num}ì—ì„œ {self.language}ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                
                # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ëŒ€ì‹  ì‚¬ìš©
                if 'ko' in trial_info and 'user_prompt' in trial_info['ko'] and trial_info['ko']['user_prompt']:
                    self.language = 'ko'  # ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë³€ê²½
                    self.prompt_template = trial_info['ko']['user_prompt']
                    print(f"âœ… ëŒ€ì‹  í•œêµ­ì–´(ko) í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì–¸ì–´ ì‚¬ìš©
                    for lang, lang_data in trial_info.items():
                        if lang != 'explanation' and 'user_prompt' in lang_data and lang_data['user_prompt']:
                            self.language = lang
                            self.prompt_template = lang_data['user_prompt']
                            print(f"âœ… ëŒ€ì‹  {lang} ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            break
                    else:
                        raise ValueError(f"{self.trial_num}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ì„ íƒëœ ì–¸ì–´ê°€ ì—†ìŒ
            print(f"âš ï¸ {self.trial_num}ì—ì„œ {self.language}ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ëŒ€ì‹  ì‚¬ìš©
            if 'ko' in trial_info and 'user_prompt' in trial_info['ko'] and trial_info['ko']['user_prompt']:
                self.language = 'ko'  # ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë³€ê²½
                self.prompt_template = trial_info['ko']['user_prompt']
                print(f"âœ… ëŒ€ì‹  í•œêµ­ì–´(ko) í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì–¸ì–´ ì‚¬ìš©
                for lang, lang_data in trial_info.items():
                    if lang != 'explanation' and 'user_prompt' in lang_data and lang_data['user_prompt']:
                        self.language = lang
                        self.prompt_template = lang_data['user_prompt']
                        print(f"âœ… ëŒ€ì‹  {lang} ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        break
                else:
                    raise ValueError(f"{self.trial_num}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"\n===== ì„ íƒëœ í”„ë¡¬í”„íŠ¸ =====")
        print(f"íŠ¸ë¼ì´ì–¼: {self.trial_num}")
        print(f"ì–¸ì–´: {self.language}")
        print(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: {self.prompt_template}")

    def optimize_prompt(self, trial_info):
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”: ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì§€ì‹œì‚¬í•­ í¬í•¨"""
        if 'user_prompt' not in trial_info[self.language]:
            return None
        
        base_prompt = trial_info[self.language]['user_prompt']
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        optimized_prompt = base_prompt
        
        # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ë‚´ìš© ì¶”ê°€
        addition = "\n\në°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì„ ì§€ì¼œì£¼ì„¸ìš”:\nìƒì„±ëœ ë‹¨ì–´ë§Œ ë°±í‹±(`) ì‚¬ì´ì— ì…ë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."
        
        if "ê·œì¹™" not in optimized_prompt:
            optimized_prompt += addition
        
        return optimized_prompt
    
    def load_source_data(self) -> None:
        """ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ"""
        # ì†ŒìŠ¤ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        source_file = Path(f"../1_preprocess/nat/{self.language}.json")
        
        try:
            with open(source_file, 'r', encoding='utf-8') as f:
                self.source_data = json.load(f)
            
            print(f"âœ… {len(self.source_data)} ì†ŒìŠ¤ ë‹¨ì–´ë¥¼ {source_file}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f"âŒ ì†ŒìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_file}")
            self.source_data = []
            print("ë¹ˆ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def set_words_per_meaning(self, count: int) -> None:
        """ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜ ì„¤ì •"""
        if count < 1:
            print("âš ï¸ ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ëŠ” ìµœì†Œ 1ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            count = 1
        
        self.words_per_meaning = count
        print(f"ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜: {self.words_per_meaning}")
    
    def prepare_model(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„"""
        # ì´ë¯¸ íŒŒì´í”„ë¼ì¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ” ê²½ìš°
        if self.pipeline is not None:
            return True
        
        # Hugging Face ë¡œê·¸ì¸ ì‹œë„ (í•„ìš”í•œ ê²½ìš°)
        model_info = AVAILABLE_MODELS[self.local_model_name]
        if model_info.get("requires_auth", False):
            if not hf_login():
                print(f"âš ï¸ ëª¨ë¸ '{self.local_model_name}'ì€(ëŠ”) ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        model_id = model_info["hf_id"]
        print(f"ğŸ”§ ëª¨ë¸ '{model_id}' ë¡œë“œ ì¤‘...")
        
        try:
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ’» ì‚¬ìš© ì¥ì¹˜: {device}")
            
            # ëª¨ë¸ í¬ê¸° ì¶”ì • (ì´ë¦„ì—ì„œ ìˆ«ì ì¶”ì¶œ)
            model_size = 0
            import re
            size_match = re.search(r'(\d+)[bB]', self.local_model_name)
            if size_match:
                model_size = int(size_match.group(1))
            
            # í° ëª¨ë¸ (7B ì´ìƒ)ì— ëŒ€í•œ ì–‘ìí™” ë° ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            if model_size >= 7:
                print(f"ğŸ” {model_size}B ì´ìƒì˜ í° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
                try:
                    # 4ë¹„íŠ¸ ì–‘ìí™” ì‹œë„
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_quant_type="nf4"
                    )
                    
                    # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„± (ì–‘ìí™” ì ìš©)
                    self.pipeline = pipeline(
                        "text-generation",
                        model=model_id,
                        model_kwargs={"quantization_config": quantization_config, "device_map": "auto"},
                        max_new_tokens=50,
                        temperature=0.7,
                        top_p=0.9,
                        top_k=50,
                        repetition_penalty=1.2,
                        torch_dtype=torch.float16 if device == "cuda" else torch.float32
                    )
                    print(f"âœ… ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ! (4ë¹„íŠ¸ ì–‘ìí™” ì ìš©)")
                    return True
                except (ImportError, Exception) as e:
                    print(f"âš ï¸ 4ë¹„íŠ¸ ì–‘ìí™” ì ìš© ì‹¤íŒ¨: {e}")
                    # ì¼ë°˜ ëª¨ë“œë¡œ ê³„ì† ì‹œë„
            
            # ì¼ë°˜ ëª¨ë“œ (ì‘ì€ ëª¨ë¸ ë˜ëŠ” ì–‘ìí™” ì‹¤íŒ¨ ì‹œ)
            self.pipeline = pipeline(
                "text-generation",
                model=model_id,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            print(f"âœ… ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ!")
            return True
        
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ë‹¤ë¥¸ ê°œë°©í˜• ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            
            # ê°œë°©í˜• ëª¨ë¸ ì‹œë„
            for model_name in ["gpt2", "bloom-560m", "opt-350m"]:
                try:
                    model_id = AVAILABLE_MODELS[model_name]["hf_id"]
                    print(f"ğŸ”„ ëŒ€ì²´ ëª¨ë¸ '{model_id}' ë¡œë“œ ì¤‘...")
                    self.pipeline = pipeline(
                        "text-generation",
                        model=model_id,
                        max_new_tokens=50
                    )
                    print(f"âœ… ëŒ€ì²´ ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ!")
                    self.local_model_name = model_name
                    return True
                except Exception as e2:
                    print(f"âŒ ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e2}")
            
            print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
    
    def extract_word(self, text: str, meaning: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ ë° ìœ íš¨ì„± ê²€ì‚¬"""
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
        prompt = self.prompt_template.format(meaning="")
        if prompt in text and len(prompt) < len(text):
            text = text[len(prompt):].strip()
        
        # ë°±í‹±(`) ë˜ëŠ” ëŒ€ê´„í˜¸([]) ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ ì‹œë„
        extracted_word = None
        
        # ë°±í‹± ì•ˆì— ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
        if '`' in text:
            parts = text.split('`')
            if len(parts) >= 3:  # `word` í˜•ì‹
                extracted_word = parts[1].strip()
        
        # ëŒ€ê´„í˜¸ ì•ˆì— ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
        if not extracted_word and '[' in text and ']' in text:
            start_idx = text.find('[')
            end_idx = text.find(']', start_idx)
            if start_idx != -1 and end_idx != -1:
                extracted_word = text[start_idx+1:end_idx].strip()
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì¤„ ì‚¬ìš©
        if not extracted_word:
            lines = text.strip().split('\n')
            extracted_word = lines[0].strip() if lines else ""
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if extracted_word:
            breakpoint()
            # 1. ë‹¨ì–´ ê¸¸ì´ ê²€ì‚¬ (ë„ˆë¬´ ê¸¸ë©´ ë¬´íš¨)
            if len(extracted_word) > 15:
                print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {extracted_word}")
                return None
            
            # 2. ì˜ë¯¸ì™€ ìœ ì‚¬ì„± ê²€ì‚¬ (ì˜ë¯¸ê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë˜ë©´ ë¬´íš¨)
            meaning_words = set(meaning.replace(',', ' ').replace('.', ' ').split())
            for word in meaning_words:
                if len(word) > 3 and word in extracted_word:  # 3ê¸€ì ì´ìƒì˜ ì˜ë¯¸ ë‹¨ì–´ê°€ í¬í•¨
                    print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ì— ì˜ë¯¸ê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë¨: {extracted_word}, í¬í•¨ë‹¨ì–´: {word}")
                    return None
            
            # 3. íŠ¹ì • ë¬´íš¨ íŒ¨í„´ ê²€ì‚¬
            invalid_patterns = ["ìƒì„±ëœ ì–´íœ˜", "ìƒì„±ëœì–´íœ˜", "ê°€ìƒ", "ìŒì„±ìƒì§•ì–´", 
                              "ì˜ˆì‹œ", "ë‹¨ì–´", "ì¼ë³¸ì–´", "í•œêµ­ì–´", "ì˜ì–´", "í”„ë‘ìŠ¤ì–´"]
            for pattern in invalid_patterns:
                if pattern in extracted_word:
                    print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ì— ë¬´íš¨ íŒ¨í„´ í¬í•¨: {pattern}")
                    return None
            
            # ìœ íš¨í•œ ë‹¨ì–´ì¸ ê²½ìš° ë°˜í™˜
            return extracted_word
        
        return None  # ì¶”ì¶œ ì‹¤íŒ¨
    
    def generate_word(self, meaning: str) -> str:
        """ì£¼ì–´ì§„ ì˜ë¯¸ì— ëŒ€í•œ ê°€ìƒ ë‹¨ì–´ ìƒì„± (ìµœëŒ€ 3ë²ˆ ì‹œë„)"""
        if not self.prepare_model():
            return ""
        
        prompt = self.prompt_template.format(meaning=meaning)
        
        # ìµœëŒ€ 3ë²ˆê¹Œì§€ ì‹œë„
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                # ì±„íŒ… í˜•ì‹ ëª¨ë¸ì¸ì§€ í™•ì¸ (Instruct ëª¨ë¸ì¸ ê²½ìš°)
                is_chat_model = any(keyword in self.local_model_name.lower() for keyword in 
                                  ["instruct", "chat", "it", "thinking"])
                
                # ë©”ì‹œì§€ ì¤€ë¹„
                if is_chat_model:
                    messages = [
                        {"role": "user", "content": prompt}
                    ]
                    
                    # ì±„íŒ… ëª¨ë¸ ì²˜ë¦¬
                    try:
                        result = self.pipeline(messages)
                        generated_text = result[0]['generated_text']
                    except Exception:
                        # ì±„íŒ… ëª¨ë“œ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œ ì‹œë„
                        result = self.pipeline(prompt)
                        generated_text = result[0]['generated_text']
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
                    result = self.pipeline(prompt)
                    generated_text = result[0]['generated_text']
                
                breakpoint()
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ ë° ìœ íš¨ì„± ê²€ì‚¬
                word = self.extract_word(generated_text, meaning)
                
                # ì¶”ì¶œ ì„±ê³µ ë° ìœ íš¨í•œ ë‹¨ì–´ì¸ ê²½ìš°
                if word:
                    print(f"âœ… ì‹œë„ {attempt+1}/{max_attempts}: ì„±ê³µì ìœ¼ë¡œ ë‹¨ì–´ ìƒì„±: {word}")
                    return word
                
                # ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„
                print(f"ğŸ”„ ì‹œë„ {attempt+1}/{max_attempts}: ìœ íš¨í•˜ì§€ ì•Šì€ ë‹¨ì–´, ì¬ì‹œë„ ì¤‘...")
                time.sleep(1)  # ì§§ì€ ëŒ€ê¸°
                
            except Exception as e:
                print(f"âŒ ì‹œë„ {attempt+1}/{max_attempts}: ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(1)
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        print(f"âŒ {max_attempts}ë²ˆ ì‹œë„ í›„ ìœ íš¨í•œ ë‹¨ì–´ ìƒì„± ì‹¤íŒ¨")
        return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    
    def generate_words(self, num_words: Optional[int] = None) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë‹¨ì–´ ìƒì„± (ê¸°ì¡´ ê²°ê³¼ ìœ ì§€ ë° ì—…ë°ì´íŠ¸)"""
        # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
        all_results = self.load_existing_results()
        
        # ìƒì„±í•  ë‹¨ì–´ ìˆ˜ ê²°ì •
        if num_words is None:
            num_words = len(self.source_data)
        else:
            num_words = min(num_words, len(self.source_data))
        
        # ì†ŒìŠ¤ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        if not self.source_data:
            print("âŒ ì†ŒìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return all_results
        
        # ì†ŒìŠ¤ ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ
        selected_data = random.sample(self.source_data, num_words)
        
        # ì˜ë¯¸ì™€ ì›ë³¸ ë‹¨ì–´ ì¶”ì¶œ
        meanings = []
        original_words = []
        
        for item in selected_data:
            # ì˜ë¯¸ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©)
            meaning = item.get('meaning', [])
            if isinstance(meaning, list) and meaning:
                meaning = meaning[0]
            elif not meaning:
                meaning = "ì˜ë¯¸ ì—†ìŒ"
            
            meanings.append(meaning)
            original_words.append(item.get('word', ''))
        
        print(f"ğŸ” {len(meanings)}ê°œ ì˜ë¯¸ì— ëŒ€í•´ ê°€ìƒ ë‹¨ì–´ ìƒì„±/ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ë³€ê²½ëœ í•­ëª© ìˆ˜ ì¶”ì 
        added_count = 0
        updated_count = 0
        
        # ê° ì˜ë¯¸ì— ëŒ€í•´ ë‹¨ì–´ ìƒì„±
        for idx, (meaning, orig_word) in enumerate(tqdm(zip(meanings, original_words), 
                                                      desc="ë‹¨ì–´ ìƒì„± ì¤‘", total=len(meanings))):
            # ì˜ë¯¸ë‹¹ ì—¬ëŸ¬ ë‹¨ì–´ ìƒì„±
            for i in range(self.words_per_meaning):
                # ì´ë¯¸ ìƒì„±ëœ í•­ëª©ì¸ì§€ í™•ì¸
                dup_idx = self.is_duplicate_entry(meaning, orig_word, all_results)
                
                # ê¸°ì¡´ í•­ëª©ì´ ìˆê³  ìƒì„±ëœ ë‹¨ì–´ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
                if dup_idx >= 0 and all_results[dup_idx].get("generated_word"):
                    print(f"ğŸ”„ ê±´ë„ˆëœ€: '{meaning}' (ì´ë¯¸ '{all_results[dup_idx]['generated_word']}'ë¡œ ìƒì„±ë¨)")
                    continue
                
                # ë‹¨ì–´ ìƒì„±
                generated_word = self.generate_word(meaning)
                
                # ê²°ê³¼ ìƒì„±
                result = {
                    "original_meaning": meaning,
                    "original_word": orig_word,
                    "generated_word": generated_word,
                    "trial": self.trial_num,
                    "model": AVAILABLE_MODELS[self.local_model_name]["hf_id"],
                    "language": self.language,
                    "words_per_meaning": self.words_per_meaning
                }
                
                # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
                if dup_idx >= 0:
                    # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
                    if generated_word:  # ìƒì„±ëœ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
                        all_results[dup_idx] = result
                        updated_count += 1
                        print(f"ğŸ”„ ì—…ë°ì´íŠ¸: '{meaning}' -> '{generated_word}'")
                else:
                    # ìƒˆ í•­ëª© ì¶”ê°€
                    all_results.append(result)
                    added_count += 1
                    if generated_word:
                        print(f"â• ì¶”ê°€: '{meaning}' -> '{generated_word}'")
                    else:
                        print(f"âš ï¸ ì¶”ê°€: '{meaning}' -> ìƒì„± ì‹¤íŒ¨")
                
                # ë³€ê²½ëœ ë‚´ìš© ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
                if (added_count + updated_count) % 10 == 0 and (added_count + updated_count) > 0:
                    self.save_results(all_results)
            
            # ê°„ê²© ë‘ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(0.5)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        self.save_results(all_results)
        
        print(f"âœ… ê°€ìƒ ë‹¨ì–´ ìƒì„± ì™„ë£Œ - ì¶”ê°€: {added_count}ê°œ, ì—…ë°ì´íŠ¸: {updated_count}ê°œ, ì´: {len(all_results)}ê°œ")
        
        return all_results
    
    def run(self, num_words: Optional[int] = None) -> List[Dict[str, Any]]:
        """ê°€ìƒ ë‹¨ì–´ ìƒì„± ì‹¤í–‰"""
        print(f"===== ê°€ìƒ ë‹¨ì–´ ìƒì„± ì‹œì‘ =====")
        print(f"ì–¸ì–´: {self.language}")
        print(f"ëª¨ë¸: {AVAILABLE_MODELS[self.local_model_name]['hf_id']}")
        print(f"íŠ¸ë¼ì´ì–¼: {self.trial_num}")
        print(f"ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜: {self.words_per_meaning}")
        
        # ëª¨ë¸ ì¤€ë¹„
        if not self.prepare_model():
            print("âŒ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨")
            return []
        
        # ë‹¨ì–´ ìƒì„±
        try:
            results = self.generate_words(num_words)
            print(f"===== ê°€ìƒ ë‹¨ì–´ ìƒì„± ì™„ë£Œ =====")
            return results
        except Exception as e:
            print(f"âŒ ë‹¨ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return []

    def load_existing_results(self) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ì— ìƒì„±ëœ ë‹¨ì–´ ê²°ê³¼ ë¡œë“œ"""
        output_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.json"
        
        if output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                print(f"âœ… ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(existing_results)}ê°œ ê²°ê³¼ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {output_file}")
                return existing_results
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print(f"ğŸ“ ìƒˆ ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤: {output_file}")
        return []

    def is_duplicate_entry(self, meaning: str, orig_word: str, existing_results: List[Dict[str, Any]]) -> int:
        """ì´ë¯¸ ìƒì„±ëœ ë‹¨ì–´ì¸ì§€ í™•ì¸í•˜ê³  ì¤‘ë³µ í•­ëª©ì˜ ì¸ë±ìŠ¤ ë°˜í™˜"""
        model_id = AVAILABLE_MODELS[self.local_model_name]["hf_id"]
        
        for idx, entry in enumerate(existing_results):
            # ì˜ë¯¸, ì›ë³¸ ë‹¨ì–´, ëª¨ë¸ì´ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
            if (entry.get("original_meaning") == meaning and 
                entry.get("original_word") == orig_word and 
                entry.get("model") == model_id and
                entry.get("trial") == self.trial_num):
                return idx
        
        # ì¤‘ë³µ ì—†ìŒ
        return -1

    def save_results(self, results: List[Dict[str, Any]]) -> None:
        """ìƒì„±ëœ ë‹¨ì–´ ê²°ê³¼ ì €ì¥ (JSON ë° CSV)"""
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        if not results:
            print("âš ï¸ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # JSON íŒŒì¼ ì €ì¥
        output_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(results)}ê°œ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {output_file}")
        
        # CSV íŒŒì¼ ì €ì¥
        csv_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.csv"
        df = pd.DataFrame(results)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ê²°ê³¼ë¥¼ CSVë¡œë„ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {csv_file}")

def setup_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì„¤ì¹˜"""
    try:
        import torch
        import transformers
        print("âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (torch, transformers)")
    except ImportError:
        print("âš ï¸ ê¸°ë³¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        import subprocess
        subprocess.check_call(["pip", "install", "torch", "transformers"])
    
    # ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import bitsandbytes
        import accelerate
        print("âœ… ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (bitsandbytes, accelerate)")
    except ImportError:
        print("âš ï¸ ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤ (ëŒ€í˜• ëª¨ë¸ì— í•„ìš”)...")
        import subprocess
        subprocess.check_call(["pip", "install", "bitsandbytes", "accelerate"])
    
    print("âœ… ëª¨ë“  í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ê°€ìƒ ë‹¨ì–´ ìƒì„± ë„êµ¬')
    parser.add_argument('--language', '-l', choices=['en', 'fr', 'ko', 'ja'], 
                        help='ì–¸ì–´ ì½”ë“œ (en/fr/ko/ja)')
    parser.add_argument('--model', '-m', default='local', choices=['local'],
                        help='ì‚¬ìš©í•  ëª¨ë¸ (localë§Œ ì§€ì›)')
    parser.add_argument('--trial', '-t', help='prompts.jsonì˜ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ (ì˜ˆ: trial1, trial2)')
    parser.add_argument('--batch-size', '-b', type=int, default=10,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--num-words', '-n', type=int, default=None,
                        help='ì²˜ë¦¬í•  ì˜ë¯¸ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)')
    parser.add_argument('--words-per-meaning', '-w', type=int, default=1,
                        help='ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 1)')
    parser.add_argument('--output-dir', '-o', type=str, default=None,
                        help='ìƒì„±ëœ ë‹¨ì–´ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--local-model', type=str, choices=list(AVAILABLE_MODELS.keys()),
                        help='ì‚¬ìš©í•  ë¡œì»¬ ëª¨ë¸')
    parser.add_argument('--debug-model', type=str, choices=list(AVAILABLE_MODELS.keys()),
                        help='ëª¨ë¸ ë””ë²„ê¹…')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë””ë²„ê¹…
    if args.debug_model:
        debug_model(args.debug_model)
        exit(0)
    
    # í•„ìˆ˜ ì¸ì í™•ì¸
    if not args.language or not args.trial:
        parser.error("--language ë° --trial ì¸ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ --local-model í•„ìš”
    if args.model == 'local' and not args.local_model:
        args.local_model = "gpt2"  # ê¸°ë³¸ê°’ ì„¤ì •
        print(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ {args.local_model}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    setup_requirements()
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = PseudoWordGenerator(
        language=args.language,
        model=args.model,
        trial_num=args.trial,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        local_model=args.local_model
    )
    
    # ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ ì„¤ì •
    if args.words_per_meaning > 1:
        generator.set_words_per_meaning(args.words_per_meaning)
    
    # ì‹¤í–‰
    generator.run(args.num_words)
