#!/usr/bin/env python3
# python pseudo_word_generation.py -l ko -m local --local-model gpt2 -t trial10 -n 5 -w 2
# python pseudo_word_generation.py -l ko -m local --local-model qwen3-32b -t trial2 -n 100 -w 2
# python pseudo_word_generation.py -l ko -m local --local-model qwen3-4b -t trial10 
# python pseudo_word_generation.py --download-model bloom-560m
# python pseudo_word_generation.py --debug-model gpt2

import os
import json
import argparse
import random
import time
import traceback
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import pandas as pd
from typing import List, Dict, Any, Optional
from huggingface_hub import login, model_info
import shutil
import psutil

# HuggingFace ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import pipeline, AutoTokenizer

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')
HUGGINGFACE_TOKEN = os.environ.get('HUGGINGFACE_TOKEN')

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ì˜ - Hugging Face ëª¨ë¸ IDë§Œ í¬í•¨
AVAILABLE_MODELS = {
    # Gemma ëª¨ë¸
    "gemma-3-4b-it": {
        "hf_id": "google/gemma-3-4b-it",
        "description": "Googleì˜ Gemma 3 4B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-12b-it": {
        "hf_id": "google/gemma-3-12b-it", 
        "description": "Googleì˜ Gemma 3 12B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    "gemma-3-27b-it": {
        "hf_id": "google/gemma-3-27b-it",
        "description": "Googleì˜ Gemma 3 27B Instruction ëª¨ë¸",
        "requires_auth": True
    },
    
    # Qwen 2.5 ëª¨ë¸
    "qwen2.5-3b": {
        "hf_id": "Qwen/Qwen2.5-3B-Instruct",
        "description": "Qwenì˜ 2.5 3B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-7b": {
        "hf_id": "Qwen/Qwen2.5-7B-Instruct",
        "description": "Qwenì˜ 2.5 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-14b": {
        "hf_id": "Qwen/Qwen2.5-14B-Instruct",
        "description": "Qwenì˜ 2.5 14B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-32b": {
        "hf_id": "Qwen/Qwen2.5-32B-Instruct",
        "description": "Qwenì˜ 2.5 32B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "qwen2.5-72b": {
        "hf_id": "Qwen/Qwen2.5-72B-Instruct",
        "description": "Qwenì˜ 2.5 72B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 ëª¨ë¸
    "qwen3-4b": {
        "hf_id": "Qwen/Qwen3-4B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b": {
        "hf_id": "Qwen/Qwen3-8B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b": {
        "hf_id": "Qwen/Qwen3-14B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b": {
        "hf_id": "Qwen/Qwen3-32B",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B ëª¨ë¸",
        "requires_auth": False
    },
    
    # Qwen 3 Thinking ëª¨ë¸
    "qwen3-4b-thinking": {
        "hf_id": "Qwen/Qwen3-4B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 4B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-8b-thinking": {
        "hf_id": "Qwen/Qwen3-8B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 8B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-14b-thinking": {
        "hf_id": "Qwen/Qwen3-14B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 14B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    "qwen3-32b-thinking": {
        "hf_id": "Qwen/Qwen3-32B-Thinking",
        "description": "Qwenì˜ 3ì„¸ëŒ€ 32B Thinking ëª¨ë¸",
        "requires_auth": False
    },
    
    # OLMo ëª¨ë¸
    "olmo-7b": {
        "hf_id": "allenai/OLMo-2-1124-7B-Instruct",
        "description": "Allen AIì˜ OLMo 7B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-13b": {
        "hf_id": "allenai/OLMo-2-1124-13B-Instruct",
        "description": "Allen AIì˜ OLMo 13B Instruct ëª¨ë¸",
        "requires_auth": False
    },
    "olmo-32b": {
        "hf_id": "allenai/OLMo-2-0325-32B-Instruct",
        "description": "Allen AIì˜ OLMo 32B Instruct ëª¨ë¸",
        "requires_auth": False
    }
}

# Hugging Face ë¡œê·¸ì¸ í•¨ìˆ˜
def hf_login() -> bool:
    """Hugging Faceì— ë¡œê·¸ì¸"""
    if HUGGINGFACE_TOKEN:
        try:
            login(token=HUGGINGFACE_TOKEN)
            print("âœ… Hugging Faceì— ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âš ï¸ Hugging Face ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì¼ë¶€ ëª¨ë¸ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return False

# ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def check_model_access(model_name: str) -> bool:
    """ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    if model_name not in AVAILABLE_MODELS:
        print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(AVAILABLE_MODELS.keys())}")
        return False
    
    model_id = AVAILABLE_MODELS[model_name]["hf_id"]
    print(f"ğŸ” ëª¨ë¸ '{model_id}' ì ‘ê·¼ì„± í™•ì¸ ì¤‘...")
    
    try:
        info = model_info(model_id)
        print(f"âœ… ëª¨ë¸ '{model_id}'ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"âŒ ëª¨ë¸ '{model_id}'ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ì´ ëª¨ë¸ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ ì ‘ê·¼ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False

# ëª¨ë¸ ë””ë²„ê¹… í•¨ìˆ˜
def debug_model(model_name: str) -> None:
    """ëª¨ë¸ ì ‘ê·¼ì„± ë””ë²„ê¹…"""
    if model_name not in AVAILABLE_MODELS:
        print(f"âŒ ëª¨ë¸ '{model_name}'ì€(ëŠ”) AVAILABLE_MODELSì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    model_info = AVAILABLE_MODELS[model_name]
    model_id = model_info["hf_id"]
    
    print(f"===== ëª¨ë¸ ë””ë²„ê¹… =====")
    print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
    print(f"Hugging Face ID: {model_id}")
    print(f"ì„¤ëª…: {model_info['description']}")
    
    # ë¡œê·¸ì¸ ì‹œë„
    hf_login()
    
    # ëª¨ë¸ ì ‘ê·¼ì„± í™•ì¸
    try:
        print("\nğŸ” ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘...")
        info = model_info(model_id)
        print(f"âœ… ëª¨ë¸ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print(f"ëª¨ë¸ íƒ€ì…: {info.modelId}")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        try:
            pipe = pipeline("text-generation", model=model_id, max_new_tokens=20)
            test_result = pipe("This is a test:")
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_result[0]['generated_text']}")
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    print("===== ë””ë²„ê¹… ì™„ë£Œ =====")

def check_disk_space(model_name: str) -> bool:
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— í•„ìš”í•œ ë””ìŠ¤í¬ ê³µê°„ì´ ì¶©ë¶„í•œì§€ í™•ì¸"""
    # ëª¨ë¸ í¬ê¸° ì¶”ì • (ë‹¨ìœ„: GB)
    model_sizes = {
        "3b": 3.0,
        "4b": 4.0,
        "7b": 7.0,
        "8b": 8.0,
        "12b": 12.0,
        "13b": 13.0,
        "14b": 14.0,
        "27b": 27.0,
        "32b": 32.0,
        "72b": 72.0
    }
    
    # ëª¨ë¸ ì´ë¦„ì—ì„œ í¬ê¸° ì¶”ì¶œ
    model_size_gb = 2.0  # ê¸°ë³¸ê°’
    for size_key, size_value in model_sizes.items():
        if size_key in model_name.lower():
            model_size_gb = size_value
            break
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ - í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
    cache_dir = os.environ.get("HUGGINGFACE_HUB_CACHE", "~/.cache/huggingface/hub")
    cache_dir = os.path.expanduser(cache_dir)
    
    # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    try:
        # ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ìˆëŠ” íŒŒí‹°ì…˜ì˜ ë‚¨ì€ ê³µê°„ í™•ì¸
        disk_usage = shutil.disk_usage(cache_dir if os.path.exists(cache_dir) else os.path.dirname(cache_dir))
        free_space_gb = disk_usage.free / (1024 ** 3)  # ë°”ì´íŠ¸ë¥¼ GBë¡œ ë³€í™˜
        
        print(f"ğŸ” í•„ìš”í•œ ë””ìŠ¤í¬ ê³µê°„: {model_size_gb:.1f}GB, ê°€ìš© ê³µê°„: {free_space_gb:.1f}GB")
        print(f"ğŸ“ ëª¨ë¸ ìºì‹œ ê²½ë¡œ: {cache_dir}")
        
        # í•„ìš”í•œ ê³µê°„ì˜ 1.5ë°°ë¥¼ í™•ë³´í–ˆëŠ”ì§€ í™•ì¸ (ì—¬ìœ  ìˆê²Œ)
        required_space = model_size_gb * 1.5
        if free_space_gb < required_space:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±! ìµœì†Œ {required_space:.1f}GBê°€ í•„ìš”í•˜ì§€ë§Œ {free_space_gb:.1f}GBë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("  1. ë¶ˆí•„ìš”í•œ íŒŒì¼ì„ ì‚­ì œí•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”.")
            print("  2. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ HF ìºì‹œë¥¼ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: rm -rf ~/.cache/huggingface")
            print("  3. ë” ì‘ì€ ëª¨ë¸(ì˜ˆ: 3B/4B í¬ê¸° ëª¨ë¸)ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            return False
        
        return True
    except Exception as e:
        print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

# ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì • í•¨ìˆ˜ ì¶”ê°€
def setup_model_cache():
    """HuggingFace ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •"""
    # ì‚¬ìš©ì ì§€ì • ìºì‹œ ë””ë ‰í† ë¦¬
    custom_cache_dir = "/scratch2/sheepswool/workspace/models"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(custom_cache_dir):
        try:
            os.makedirs(custom_cache_dir, exist_ok=True)
            print(f"âœ… ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {custom_cache_dir}")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["HUGGINGFACE_HUB_CACHE"] = custom_cache_dir
    os.environ["TRANSFORMERS_CACHE"] = custom_cache_dir
    os.environ["HF_HOME"] = custom_cache_dir
    
    # íŒŒì¼ ê¶Œí•œ í™•ì¸
    try:
        test_file = os.path.join(custom_cache_dir, "test_write.txt")
        with open(test_file, 'w') as f:
            f.write("Test write permission")
        os.remove(test_file)
        print(f"âœ… ìºì‹œ ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œ í™•ì¸ë¨")
        
        # í˜„ì¬ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        disk_usage = shutil.disk_usage(custom_cache_dir)
        free_space_gb = disk_usage.free / (1024 ** 3)
        print(f"âœ… ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ìš© ê³µê°„: {free_space_gb:.1f}GB")
        
        return True
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ ë˜ëŠ” ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

class PseudoWordGenerator:
    def __init__(self, language: str, model: str = "local", trial_num: str = "trial1", 
                 batch_size: int = 10, output_dir: Optional[str] = None, 
                 local_model: Optional[str] = "gpt2"):
        """ê°€ìƒ ë‹¨ì–´ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.language = language.lower()
        if self.language not in ['en', 'ja', 'ko', 'fr']:
            raise ValueError("ì–¸ì–´ëŠ” 'en', 'ja', 'ko', 'fr' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        self.model_type = model.lower()
        if self.model_type != "local":
            self.model_type = "local"  # OpenAI ëŒ€ì‹  ë¬´ì¡°ê±´ local ëª¨ë¸ ì‚¬ìš©
            print("âš ï¸ OpenAI API ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        self.trial_num = trial_num
        self.batch_size = batch_size
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = Path(f"../0_raw/art/{self.language}")
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œì»¬ ëª¨ë¸ ì„¤ì •
        self.local_model_name = local_model or "gpt2"
        if self.local_model_name not in AVAILABLE_MODELS:
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {self.local_model_name}")
            print(f"gpt2 ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.local_model_name = "gpt2"
        
        # ëª¨ë¸ íŒŒì´í”„ë¼ì¸ (ì²˜ìŒì—ëŠ” None, í•„ìš”í•  ë•Œ ë¡œë“œ)
        self.pipeline = None
        
        # í”„ë¡¬í”„íŠ¸ ë° ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        self.load_prompts()
        self.load_source_data()
        
        # ìƒì„± ì„¤ì •
        self.words_per_meaning = 1  # ê¸°ë³¸ê°’: ì˜ë¯¸ë‹¹ 1ê°œ ë‹¨ì–´
    
    def load_prompts(self) -> None:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        # prompts.json íŒŒì¼ ê²½ë¡œ
        prompts_file = Path("../../analysis/experiments/prompts.json")
        
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts_data = json.load(f)
        
        # generation í‚¤ ì•„ë˜ì˜ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
        generation_prompts = prompts_data.get('generation', {})
        
        # íŠ¸ë¼ì´ì–¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if self.trial_num not in generation_prompts:
            print(f"âŒ generation í”„ë¡¬í”„íŠ¸ì—ì„œ {self.trial_num}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            available_trials = list(generation_prompts.keys())
            if available_trials:
                self.trial_num = available_trials[0]
                print(f"ëŒ€ì‹  {self.trial_num}ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¸ë¼ì´ì–¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŠ¸ë¼ì´ì–¼ ì •ë³´ ë¡œë“œ
        trial_info = generation_prompts[self.trial_num]
        
        # ì„¤ëª… ì¶œë ¥ (ìˆìœ¼ë©´)
        if 'explanation' in trial_info:
            print(f"\n===== Trial {self.trial_num} ì •ë³´ =====")
            for key, value in trial_info['explanation'].items():
                print(f"  {key}: {value}")
        
        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        if self.language in trial_info:
            if 'user_prompt' in trial_info[self.language] and trial_info[self.language]['user_prompt']:
                # í”„ë¡¬í”„íŠ¸ ìµœì í™”
                optimized_prompt = self.optimize_prompt(trial_info)
                if optimized_prompt:
                    self.prompt_template = optimized_prompt
                    print(f"âœ… {self.language} ì–¸ì–´ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ì—¬ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                else:
                    self.prompt_template = trial_info[self.language]['user_prompt']
                    print(f"âœ… {self.language} ì–¸ì–´ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            else:
                # ì„ íƒëœ ì–¸ì–´ì— í”„ë¡¬í”„íŠ¸ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ
                print(f"âš ï¸ {self.trial_num}ì—ì„œ {self.language}ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                
                # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ëŒ€ì‹  ì‚¬ìš©
                if 'ko' in trial_info and 'user_prompt' in trial_info['ko'] and trial_info['ko']['user_prompt']:
                    self.language = 'ko'  # ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë³€ê²½
                    self.prompt_template = trial_info['ko']['user_prompt']
                    print(f"âœ… ëŒ€ì‹  í•œêµ­ì–´(ko) í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì–¸ì–´ ì‚¬ìš©
                    for lang, lang_data in trial_info.items():
                        if lang != 'explanation' and 'user_prompt' in lang_data and lang_data['user_prompt']:
                            self.language = lang
                            self.prompt_template = lang_data['user_prompt']
                            print(f"âœ… ëŒ€ì‹  {lang} ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            break
                    else:
                        raise ValueError(f"{self.trial_num}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ì„ íƒëœ ì–¸ì–´ê°€ ì—†ìŒ
            print(f"âš ï¸ {self.trial_num}ì—ì„œ {self.language}ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ëŒ€ì‹  ì‚¬ìš©
            if 'ko' in trial_info and 'user_prompt' in trial_info['ko'] and trial_info['ko']['user_prompt']:
                self.language = 'ko'  # ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë³€ê²½
                self.prompt_template = trial_info['ko']['user_prompt']
                print(f"âœ… ëŒ€ì‹  í•œêµ­ì–´(ko) í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì–¸ì–´ ì‚¬ìš©
                for lang, lang_data in trial_info.items():
                    if lang != 'explanation' and 'user_prompt' in lang_data and lang_data['user_prompt']:
                        self.language = lang
                        self.prompt_template = lang_data['user_prompt']
                        print(f"âœ… ëŒ€ì‹  {lang} ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        break
                else:
                    raise ValueError(f"{self.trial_num}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"\n===== ì„ íƒëœ í”„ë¡¬í”„íŠ¸ =====")
        print(f"íŠ¸ë¼ì´ì–¼: {self.trial_num}")
        print(f"ì–¸ì–´: {self.language}")
        print(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: {self.prompt_template}")

    def optimize_prompt(self, trial_info):
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”: ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì§€ì‹œì‚¬í•­ í¬í•¨"""
        if 'user_prompt' not in trial_info[self.language]:
            return None
        
        base_prompt = trial_info[self.language]['user_prompt']
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        optimized_prompt = base_prompt
        
        # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ë‚´ìš© ì¶”ê°€
        addition = "\n\në°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì„ ì§€ì¼œì£¼ì„¸ìš”:\nìƒì„±ëœ ë‹¨ì–´ë§Œ ë°±í‹±(`) ì‚¬ì´ì— ì…ë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."
        
        if "ê·œì¹™" not in optimized_prompt:
            optimized_prompt += addition
        
        return optimized_prompt
    
    def load_source_data(self) -> None:
        """ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ"""
        # ì†ŒìŠ¤ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        source_file = Path(f"../1_preprocess/nat/{self.language}.json")
        
        try:
            with open(source_file, 'r', encoding='utf-8') as f:
                self.source_data = json.load(f)
            
            print(f"âœ… {len(self.source_data)} ì†ŒìŠ¤ ë‹¨ì–´ë¥¼ {source_file}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f"âŒ ì†ŒìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_file}")
            self.source_data = []
            print("ë¹ˆ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def set_words_per_meaning(self, count: int) -> None:
        """ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜ ì„¤ì •"""
        if count < 1:
            print("âš ï¸ ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ëŠ” ìµœì†Œ 1ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            count = 1
        
        self.words_per_meaning = count
        print(f"ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜: {self.words_per_meaning}")
    
    def prepare_model(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„"""
        # ì´ë¯¸ íŒŒì´í”„ë¼ì¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ” ê²½ìš°
        if self.pipeline is not None:
            return True
        
        # ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
        if not setup_model_cache():
            print("âš ï¸ ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
        
        # Hugging Face ë¡œê·¸ì¸ ì‹œë„ (í•„ìš”í•œ ê²½ìš°)
        model_info = AVAILABLE_MODELS[self.local_model_name]
        if model_info.get("requires_auth", False):
            if not hf_login():
                print(f"âš ï¸ ëª¨ë¸ '{self.local_model_name}'ì€(ëŠ”) ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        model_id = model_info["hf_id"]
        
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì´ì œ ìƒˆ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        if not check_disk_space(self.local_model_name):
            print(f"âŒ ëª¨ë¸ '{model_id}' ë¡œë“œ ì‹¤íŒ¨: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±")
            return False
        
        print(f"ğŸ”§ ëª¨ë¸ '{model_id}' ë¡œë“œ ì¤‘...")
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • - ë¡œì»¬ ìºì‹œë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
            os.environ["HF_HUB_OFFLINE"] = "0"
            os.environ["TRANSFORMERS_OFFLINE"] = "0"
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ’» ì‚¬ìš© ì¥ì¹˜: {device}")
            
            # ëª¨ë¸ í¬ê¸° ì¶”ì • (ì´ë¦„ì—ì„œ ìˆ«ì ì¶”ì¶œ)
            model_size = 0
            import re
            size_match = re.search(r'(\d+)[bB]', self.local_model_name)
            if size_match:
                model_size = int(size_match.group(1))
            
            # CUDA ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
            if device == "cuda":
                free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()
                free_memory_gb = free_memory / (1024**3)
                print(f"ğŸ” CUDA ê°€ìš© ë©”ëª¨ë¦¬: {free_memory_gb:.2f}GB")
                
                if model_size > 0 and free_memory_gb < model_size/2:
                    print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì†Œ {model_size/2:.1f}GB ê¶Œì¥ (í˜„ì¬ {free_memory_gb:.2f}GB)")
            
            # í° ëª¨ë¸ (7B ì´ìƒ)ì— ëŒ€í•œ ì–‘ìí™” ë° ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            if model_size >= 7:
                print(f"ğŸ” {model_size}B ì´ìƒì˜ í° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
                
                try:
                    # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • - í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    cache_dir = os.environ.get("HUGGINGFACE_HUB_CACHE", None)
                    
                    # 4ë¹„íŠ¸ ì–‘ìí™” ì‹œë„
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_quant_type="nf4"
                    )
                    
                    # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„± (ì–‘ìí™” ì ìš©)
                    from transformers import AutoModelForCausalLM
                    
                    # ëª¨ë¸ ë¡œë“œ ì‹œ ë” ë§ì€ ì˜µì…˜ ì œê³µ
                    print(f"ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                    self.pipeline = pipeline(
                        "text-generation",
                        model=model_id,
                        model_kwargs={
                            "quantization_config": quantization_config, 
                            "device_map": "auto",
                            "cache_dir": cache_dir,
                            "low_cpu_mem_usage": True,
                            "torch_dtype": torch.float16 if device == "cuda" else torch.float32
                        },
                        max_new_tokens=50,
                        temperature=0.7,
                        top_p=0.9,
                        repetition_penalty=1.2
                    )
                    print(f"âœ… ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ! (4ë¹„íŠ¸ ì–‘ìí™” ì ìš©)")
                    return True
                except (ImportError, Exception) as e:
                    print(f"âš ï¸ 4ë¹„íŠ¸ ì–‘ìí™” ì ìš© ì‹¤íŒ¨: {e}")
                    # ì¼ë°˜ ëª¨ë“œë¡œ ê³„ì† ì‹œë„
            
            # ì¼ë°˜ ëª¨ë“œ (ì‘ì€ ëª¨ë¸ ë˜ëŠ” ì–‘ìí™” ì‹¤íŒ¨ ì‹œ)
            cache_dir = os.environ.get("HUGGINGFACE_HUB_CACHE", None)
            self.pipeline = pipeline(
                "text-generation",
                model=model_id,
                model_kwargs={
                    "cache_dir": cache_dir,
                    "low_cpu_mem_usage": True
                },
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            print(f"âœ… ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ!")
            return True
        
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ìì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•´ê²° ë°©ë²• ì œê³µ
            print("\n==== ë¬¸ì œ í•´ê²° ë°©ë²• ====")
            print("1. ë””ìŠ¤í¬ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”.")
            print("2. ë” ì‘ì€ ëª¨ë¸(3B/4B)ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            print("3. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ HF ìºì‹œë¥¼ ë¹„ìš°ì„¸ìš”: rm -rf ~/.cache/huggingface")
            print("4. ë‹¤ë¥¸ ê²½ë¡œë¥¼ ìºì‹œë¡œ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   export HUGGINGFACE_HUB_CACHE=/path/with/more/space")
            print("5. ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹œë„í•˜ì„¸ìš”:")
            print(f"   python -c \"from huggingface_hub import snapshot_download; snapshot_download('{model_id}')\"")
            print("===========================")
            
            print("âŒ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨")
            return False
    
    def extract_word(self, text: str, meaning: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ ë° ìœ íš¨ì„± ê²€ì‚¬"""
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
        prompt = self.prompt_template.format(meaning="")
        if prompt in text and len(prompt) < len(text):
            text = text[len(prompt):].strip()
        
        # ë°±í‹±(`) ë˜ëŠ” ëŒ€ê´„í˜¸([]) ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ ì‹œë„
        extracted_word = None
        
        # ë°±í‹± ì•ˆì— ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
        if '`' in text:
            parts = text.split('`')
            if len(parts) >= 3:  # `word` í˜•ì‹
                extracted_word = parts[1].strip()
        
        # ëŒ€ê´„í˜¸ ì•ˆì— ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
        if not extracted_word and '[' in text and ']' in text:
            start_idx = text.find('[')
            end_idx = text.find(']', start_idx)
            if start_idx != -1 and end_idx != -1:
                extracted_word = text[start_idx+1:end_idx].strip()
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì¤„ ì‚¬ìš©
        if not extracted_word:
            lines = text.strip().split('\n')
            extracted_word = lines[0].strip() if lines else ""
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if extracted_word:
            # 1. ë‹¨ì–´ ê¸¸ì´ ê²€ì‚¬ (ë„ˆë¬´ ê¸¸ë©´ ë¬´íš¨)
            if len(extracted_word) > 15:
                print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {extracted_word}")
                return None
            
            # 2. ì˜ë¯¸ì™€ ìœ ì‚¬ì„± ê²€ì‚¬ (ì˜ë¯¸ê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë˜ë©´ ë¬´íš¨)
            meaning_words = set(meaning.replace(',', ' ').replace('.', ' ').split())
            for word in meaning_words:
                if len(word) > 3 and word in extracted_word:  # 3ê¸€ì ì´ìƒì˜ ì˜ë¯¸ ë‹¨ì–´ê°€ í¬í•¨
                    print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ì— ì˜ë¯¸ê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë¨: {extracted_word}, í¬í•¨ë‹¨ì–´: {word}")
                    return None
            
            # 3. íŠ¹ì • ë¬´íš¨ íŒ¨í„´ ê²€ì‚¬
            invalid_patterns = ["ìƒì„±ëœ ì–´íœ˜", "ìƒì„±ëœì–´íœ˜", "ê°€ìƒ", "ìŒì„±ìƒì§•ì–´", 
                              "ì˜ˆì‹œ", "ë‹¨ì–´", "ì¼ë³¸ì–´", "í•œêµ­ì–´", "ì˜ì–´", "í”„ë‘ìŠ¤ì–´"]
            for pattern in invalid_patterns:
                if pattern in extracted_word:
                    print(f"âš ï¸ ìƒì„±ëœ ë‹¨ì–´ì— ë¬´íš¨ íŒ¨í„´ í¬í•¨: {pattern}")
                    return None
            
            # ìœ íš¨í•œ ë‹¨ì–´ì¸ ê²½ìš° ë°˜í™˜
            return extracted_word
        
        return None  # ì¶”ì¶œ ì‹¤íŒ¨
    
    def generate_words(self, num_words: Optional[int] = None) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë‹¨ì–´ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ì—†ì´ í•œ ë²ˆì— í•˜ë‚˜ì”©)"""
        # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
        all_results = self.load_existing_results()
        
        # ìƒì„±í•  ë‹¨ì–´ ìˆ˜ ê²°ì •
        if num_words is None:
            num_words = len(self.source_data)
        else:
            num_words = min(num_words, len(self.source_data))
        
        # ì†ŒìŠ¤ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        if not self.source_data:
            print("âŒ ì†ŒìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return all_results
        
        # ì†ŒìŠ¤ ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ
        selected_data = random.sample(self.source_data, num_words)
        
        # ì˜ë¯¸ì™€ ì›ë³¸ ë‹¨ì–´ ì¶”ì¶œ
        meanings = []
        original_words = []
        
        for item in selected_data:
            # ì˜ë¯¸ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©)
            meaning = item.get('meaning', [])
            if isinstance(meaning, list) and meaning:
                meaning = meaning[0]
            elif not meaning:
                meaning = "ì˜ë¯¸ ì—†ìŒ"
            
            meanings.append(meaning)
            original_words.append(item.get('word', ''))
        
        print(f"ğŸ” {len(meanings)}ê°œ ì˜ë¯¸ì— ëŒ€í•´ ê°€ìƒ ë‹¨ì–´ ìƒì„±/ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ë³€ê²½ëœ í•­ëª© ìˆ˜ ì¶”ì 
        added_count = 0
        updated_count = 0
        
        # ê° ì˜ë¯¸ì— ëŒ€í•´ ë‹¨ì–´ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ì—†ì´ í•œ ë²ˆì— í•˜ë‚˜ì”©)
        for idx, (meaning, orig_word) in enumerate(tqdm(zip(meanings, original_words), 
                                                  desc="ë‹¨ì–´ ìƒì„± ì¤‘", total=len(meanings))):
            # ì˜ë¯¸ë‹¹ ì—¬ëŸ¬ ë‹¨ì–´ ìƒì„±
            for i in range(self.words_per_meaning):
                # ì´ë¯¸ ìƒì„±ëœ í•­ëª©ì¸ì§€ í™•ì¸
                dup_idx = self.is_duplicate_entry(meaning, orig_word, all_results)
                
                # ê¸°ì¡´ í•­ëª©ì´ ìˆê³  ìƒì„±ëœ ë‹¨ì–´ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
                if dup_idx >= 0 and all_results[dup_idx].get("generated_word"):
                    print(f"ğŸ”„ ê±´ë„ˆëœ€: '{meaning}' (ì´ë¯¸ '{all_results[dup_idx]['generated_word']}'ë¡œ ìƒì„±ë¨)")
                    continue
                
                # ë‹¨ì–´ ìƒì„± (ìµœëŒ€ 3ë²ˆ ì‹œë„)
                generated_word = ""
                for attempt in range(3):
                    word = self.generate_word(meaning)
                    if word:
                        generated_word = word
                        print(f"âœ… {attempt+1}ë²ˆì§¸ ì‹œë„ì—ì„œ ìƒì„± ì„±ê³µ: '{word}'")
                        break
                    print(f"âš ï¸ {attempt+1}ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
                    time.sleep(0.5)
                
                # ê²°ê³¼ ìƒì„±
                result = {
                    "original_meaning": meaning,
                    "original_word": orig_word,
                    "generated_word": generated_word,
                    "trial": self.trial_num,
                    "model": AVAILABLE_MODELS[self.local_model_name]["hf_id"],
                    "language": self.language,
                    "words_per_meaning": self.words_per_meaning
                }
                
                # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
                if dup_idx >= 0:
                    # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
                    if generated_word:  # ìƒì„±ëœ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
                        all_results[dup_idx] = result
                        updated_count += 1
                        print(f"ğŸ”„ ì—…ë°ì´íŠ¸: '{meaning}' -> '{generated_word}'")
                else:
                    # ìƒˆ í•­ëª© ì¶”ê°€
                    all_results.append(result)
                    added_count += 1
                    if generated_word:
                        print(f"â• ì¶”ê°€: '{meaning}' -> '{generated_word}'")
                    else:
                        print(f"âš ï¸ ì¶”ê°€: '{meaning}' -> ìƒì„± ì‹¤íŒ¨")
                
                # ë³€ê²½ëœ ë‚´ìš© ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
                if (added_count + updated_count) % 10 == 0 and (added_count + updated_count) > 0:
                    self.save_results(all_results)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        self.save_results(all_results)
        
        print(f"âœ… ê°€ìƒ ë‹¨ì–´ ìƒì„± ì™„ë£Œ - ì¶”ê°€: {added_count}ê°œ, ì—…ë°ì´íŠ¸: {updated_count}ê°œ, ì´: {len(all_results)}ê°œ")
        
        return all_results
    
    def generate_word(self, meaning: str) -> str:
        """ì£¼ì–´ì§„ ì˜ë¯¸ì— ëŒ€í•œ ê°€ìƒ ë‹¨ì–´ ìƒì„±"""
        if not self.prepare_model():
            return ""
        
        prompt = self.prompt_template.format(meaning=meaning)
        
        try:
            # ì±„íŒ… í˜•ì‹ ëª¨ë¸ì¸ì§€ í™•ì¸
            is_chat_model = any(keyword in self.local_model_name.lower() for keyword in 
                              ["instruct", "chat", "it", "thinking"])
            
            # ê²°ê³¼ ìƒì„±
            if is_chat_model:
                messages = [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê°€ìƒ ì–¸ì–´ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì˜ë¯¸ì— ë§ëŠ” ì§§ê³  ë…ì°½ì ì¸ ìŒì„±ìƒì§•ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
                
                try:
                    result = self.pipeline(messages, max_new_tokens=50)
                    generated_text = result[0]['generated_text']
                except Exception as e:
                    print(f"âš ï¸ ì±„íŒ… ëª¨ë“œ ì—ëŸ¬: {e}, ì¼ë°˜ í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹œë„")
                    result = self.pipeline(prompt, max_new_tokens=50)
                    generated_text = result[0]['generated_text']
            else:
                result = self.pipeline(prompt, max_new_tokens=50)
                generated_text = result[0]['generated_text']
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            word = self.extract_word(generated_text, meaning)
            if word:
                print(f"ğŸ”¤ ìƒì„±ëœ ë‹¨ì–´: {word} (ì˜ë¯¸: {meaning})")
                return word
            
            return ""
        except Exception as e:
            print(f"âŒ ë‹¨ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def run(self, num_words: Optional[int] = None) -> List[Dict[str, Any]]:
        """ê°€ìƒ ë‹¨ì–´ ìƒì„± ì‹¤í–‰"""
        print(f"===== ê°€ìƒ ë‹¨ì–´ ìƒì„± ì‹œì‘ =====")
        print(f"ì–¸ì–´: {self.language}")
        print(f"ëª¨ë¸: {AVAILABLE_MODELS[self.local_model_name]['hf_id']}")
        print(f"íŠ¸ë¼ì´ì–¼: {self.trial_num}")
        print(f"ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜: {self.words_per_meaning}")
        
        # ëª¨ë¸ ì¤€ë¹„
        if not self.prepare_model():
            print("âŒ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨")
            return []
        
        # ë‹¨ì–´ ìƒì„±
        try:
            results = self.generate_words(num_words)
            print(f"===== ê°€ìƒ ë‹¨ì–´ ìƒì„± ì™„ë£Œ =====")
            return results
        except Exception as e:
            print(f"âŒ ë‹¨ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return []

    def load_existing_results(self) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ì— ìƒì„±ëœ ë‹¨ì–´ ê²°ê³¼ ë¡œë“œ"""
        output_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.json"
        
        if output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                print(f"âœ… ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(existing_results)}ê°œ ê²°ê³¼ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {output_file}")
                return existing_results
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print(f"ğŸ“ ìƒˆ ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤: {output_file}")
        return []

    def is_duplicate_entry(self, meaning: str, orig_word: str, existing_results: List[Dict[str, Any]]) -> int:
        """ì´ë¯¸ ìƒì„±ëœ ë‹¨ì–´ì¸ì§€ í™•ì¸í•˜ê³  ì¤‘ë³µ í•­ëª©ì˜ ì¸ë±ìŠ¤ ë°˜í™˜"""
        model_id = AVAILABLE_MODELS[self.local_model_name]["hf_id"]
        
        for idx, entry in enumerate(existing_results):
            # ì˜ë¯¸, ì›ë³¸ ë‹¨ì–´, ëª¨ë¸ì´ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
            if (entry.get("original_meaning") == meaning and 
                entry.get("original_word") == orig_word and 
                entry.get("model") == model_id and
                entry.get("trial") == self.trial_num):
                return idx
        
        # ì¤‘ë³µ ì—†ìŒ
        return -1

    def save_results(self, results: List[Dict[str, Any]]) -> None:
        """ìƒì„±ëœ ë‹¨ì–´ ê²°ê³¼ ì €ì¥ (JSON ë° CSV)"""
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        if not results:
            print("âš ï¸ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # JSON íŒŒì¼ ì €ì¥
        output_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(results)}ê°œ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {output_file}")
        
        # CSV íŒŒì¼ ì €ì¥
        csv_file = self.output_dir / f"pseudo_words_{self.language}_{self.trial_num[5:]}.csv"
        df = pd.DataFrame(results)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ê²°ê³¼ë¥¼ CSVë¡œë„ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {csv_file}")

def setup_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì„¤ì¹˜"""
    try:
        import torch
        import transformers
        print("âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (torch, transformers)")
    except ImportError:
        print("âš ï¸ ê¸°ë³¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        import subprocess
        subprocess.check_call(["pip", "install", "torch", "transformers"])
    
    # ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import bitsandbytes
        import accelerate
        print("âœ… ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ (bitsandbytes, accelerate)")
    except ImportError:
        print("âš ï¸ ì–‘ìí™” ì§€ì› íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤ (ëŒ€í˜• ëª¨ë¸ì— í•„ìš”)...")
        import subprocess
        subprocess.check_call(["pip", "install", "bitsandbytes", "accelerate"])
    
    print("âœ… ëª¨ë“  í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # ë¨¼ì € ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
    setup_model_cache()
    
    parser = argparse.ArgumentParser(description='ê°€ìƒ ë‹¨ì–´ ìƒì„± ë„êµ¬')
    parser.add_argument('--language', '-l', choices=['en', 'fr', 'ko', 'ja'], 
                        help='ì–¸ì–´ ì½”ë“œ (en/fr/ko/ja)')
    parser.add_argument('--model', '-m', default='local', choices=['local'],
                        help='ì‚¬ìš©í•  ëª¨ë¸ (localë§Œ ì§€ì›)')
    parser.add_argument('--trial', '-t', help='prompts.jsonì˜ íŠ¸ë¼ì´ì–¼ ë²ˆí˜¸ (ì˜ˆ: trial1, trial2)')
    parser.add_argument('--batch-size', '-b', type=int, default=10,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--num-words', '-n', type=int, default=None,
                        help='ì²˜ë¦¬í•  ì˜ë¯¸ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)')
    parser.add_argument('--words-per-meaning', '-w', type=int, default=1,
                        help='ì˜ë¯¸ë‹¹ ìƒì„±í•  ë‹¨ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 1)')
    parser.add_argument('--output-dir', '-o', type=str, default=None,
                        help='ìƒì„±ëœ ë‹¨ì–´ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--local-model', type=str, choices=list(AVAILABLE_MODELS.keys()),
                        help='ì‚¬ìš©í•  ë¡œì»¬ ëª¨ë¸')
    parser.add_argument('--debug-model', type=str, choices=list(AVAILABLE_MODELS.keys()),
                        help='ëª¨ë¸ ë””ë²„ê¹…')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë””ë²„ê¹…
    if args.debug_model:
        debug_model(args.debug_model)
        exit(0)
    
    # í•„ìˆ˜ ì¸ì í™•ì¸
    if not args.language or not args.trial:
        parser.error("--language ë° --trial ì¸ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ --local-model í•„ìš”
    if args.model == 'local' and not args.local_model:
        args.local_model = "gpt2"  # ê¸°ë³¸ê°’ ì„¤ì •
        print(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ {args.local_model}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    setup_requirements()
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = PseudoWordGenerator(
        language=args.language,
        model=args.model,
        trial_num=args.trial,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        local_model=args.local_model
    )
    
    # ì˜ë¯¸ë‹¹ ë‹¨ì–´ ìˆ˜ ì„¤ì •
    if args.words_per_meaning > 1:
        generator.set_words_per_meaning(args.words_per_meaning)
    
    # ì‹¤í–‰
    generator.run(args.num_words)
